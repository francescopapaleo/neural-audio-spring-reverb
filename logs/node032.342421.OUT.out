#-----------------------------------------------------------------------#
                     Initializing training process
-------------------------------------------------------------------------

Torch version: 1.11.0 ------ Selected Device: cuda:0
Sample Rate: 16000 Hz ------  Crop Lenght: 3200 samples
-------------------------------------------------------------------------
Found 4 files in data/plate-spring/spring
Using dry_train.h5 and wet_train.h5 for train split.
-------------------------------------------------------------------------
Parameters: 247.688 k
Receptive field: 54611 samples or 3413.2 ms
-------------------------------------------------------------------------
Validation Loss Decreased(inf--->155.527648) Saving model ...
Epoch 1 		 Validation Loss: 2.777279, 		 Training Loss: 103.067265Validation Loss Decreased(155.527648--->81.836103) Saving model ...
Epoch 2 		 Validation Loss: 1.461359, 		 Training Loss: 1.613067Epoch 3 		 Validation Loss: 1.543947, 		 Training Loss: 1.682763Epoch 4 		 Validation Loss: 1.523404, 		 Training Loss: 1.481786Epoch 5 		 Validation Loss: 1.532531, 		 Training Loss: 1.468167Validation Loss Decreased(81.836103--->75.101400) Saving model ...
Epoch 6 		 Validation Loss: 1.341096, 		 Training Loss: 1.470207Epoch 7 		 Validation Loss: 1.399959, 		 Training Loss: 1.350496Epoch 8 		 Validation Loss: 1.359958, 		 Training Loss: 1.373222Validation Loss Decreased(75.101400--->72.837853) Saving model ...
Epoch 9 		 Validation Loss: 1.300676, 		 Training Loss: 1.314383Epoch 10 		 Validation Loss: 1.300974, 		 Training Loss: 1.331511Validation Loss Decreased(72.837853--->72.328094) Saving model ...
Epoch 11 		 Validation Loss: 1.291573, 		 Training Loss: 1.292528Validation Loss Decreased(72.328094--->72.220991) Saving model ...
Epoch 12 		 Validation Loss: 1.289661, 		 Training Loss: 1.277417Epoch 13 		 Validation Loss: 1.289907, 		 Training Loss: 1.293662Epoch 14 		 Validation Loss: 1.297546, 		 Training Loss: 1.281005Epoch 15 		 Validation Loss: 1.306313, 		 Training Loss: 1.288835Epoch 16 		 Validation Loss: 1.293749, 		 Training Loss: 1.295941Epoch 17 		 Validation Loss: 1.291368, 		 Training Loss: 1.279079Epoch 18 		 Validation Loss: 1.291257, 		 Training Loss: 1.284236Validation Loss Decreased(72.220991--->71.887082) Saving model ...
Epoch 19 		 Validation Loss: 1.283698, 		 Training Loss: 1.284137Epoch 20 		 Validation Loss: 1.291369, 		 Training Loss: 1.281244Epoch 21 		 Validation Loss: 1.285326, 		 Training Loss: 1.276037Epoch 22 		 Validation Loss: 1.287536, 		 Training Loss: 1.274954Epoch 23 		 Validation Loss: 1.286277, 		 Training Loss: 1.270774Validation Loss Decreased(71.887082--->71.880866) Saving model ...
Epoch 24 		 Validation Loss: 1.283587, 		 Training Loss: 1.272558Epoch 25 		 Validation Loss: 1.284139, 		 Training Loss: 1.270111Epoch 26 		 Validation Loss: 1.287349, 		 Training Loss: 1.275626Validation Loss Decreased(71.880866--->71.691399) Saving model ...
Epoch 27 		 Validation Loss: 1.280204, 		 Training Loss: 1.279518Epoch 28 		 Validation Loss: 1.281319, 		 Training Loss: 1.268288Epoch 29 		 Validation Loss: 1.282146, 		 Training Loss: 1.260416Epoch 30 		 Validation Loss: 1.282691, 		 Training Loss: 1.267871Epoch 31 		 Validation Loss: 1.281671, 		 Training Loss: 1.261577Epoch 32 		 Validation Loss: 1.281272, 		 Training Loss: 1.277482Validation Loss Decreased(71.691399--->71.657327) Saving model ...
Epoch 33 		 Validation Loss: 1.279595, 		 Training Loss: 1.270009Epoch 34 		 Validation Loss: 1.283015, 		 Training Loss: 1.265805Validation Loss Decreased(71.657327--->71.587759) Saving model ...
Epoch 35 		 Validation Loss: 1.278353, 		 Training Loss: 1.276038Validation Loss Decreased(71.587759--->71.574868) Saving model ...
Epoch 36 		 Validation Loss: 1.278123, 		 Training Loss: 1.282551Validation Loss Decreased(71.574868--->71.564877) Saving model ...
Epoch 37 		 Validation Loss: 1.277944, 		 Training Loss: 1.262367Epoch 38 		 Validation Loss: 1.278408, 		 Training Loss: 1.285290Epoch 39 		 Validation Loss: 1.278361, 		 Training Loss: 1.270536Epoch 40 		 Validation Loss: 1.278553, 		 Training Loss: 1.270617Epoch 41 		 Validation Loss: 1.277990, 		 Training Loss: 1.262832Validation Loss Decreased(71.564877--->71.552662) Saving model ...
Epoch 42 		 Validation Loss: 1.277726, 		 Training Loss: 1.286517Epoch 43 		 Validation Loss: 1.278122, 		 Training Loss: 1.267646Epoch 44 		 Validation Loss: 1.278059, 		 Training Loss: 1.262667Epoch 45 		 Validation Loss: 1.278032, 		 Training Loss: 1.263096Epoch 46 		 Validation Loss: 1.278096, 		 Training Loss: 1.267254Epoch 47 		 Validation Loss: 1.278213, 		 Training Loss: 1.261431Epoch 48 		 Validation Loss: 1.278304, 		 Training Loss: 1.258338Epoch 49 		 Validation Loss: 1.278400, 		 Training Loss: 1.264698Epoch 50 		 Validation Loss: 1.278202, 		 Training Loss: 1.270275Epoch 51 		 Validation Loss: 1.278128, 		 Training Loss: 1.278949Epoch 52 		 Validation Loss: 1.278157, 		 Training Loss: 1.260499Epoch 53 		 Validation Loss: 1.278160, 		 Training Loss: 1.264236Epoch 54 		 Validation Loss: 1.278095, 		 Training Loss: 1.267842Epoch 55 		 Validation Loss: 1.278080, 		 Training Loss: 1.269659Epoch 56 		 Validation Loss: 1.278077, 		 Training Loss: 1.271228Epoch 57 		 Validation Loss: 1.278088, 		 Training Loss: 1.267897Epoch 58 		 Validation Loss: 1.278078, 		 Training Loss: 1.273827Epoch 59 		 Validation Loss: 1.278069, 		 Training Loss: 1.262108Epoch 60 		 Validation Loss: 1.278088, 		 Training Loss: 1.263384Epoch 61 		 Validation Loss: 1.278094, 		 Training Loss: 1.261671Epoch 62 		 Validation Loss: 1.278047, 		 Training Loss: 1.279403Epoch 63 		 Validation Loss: 1.278041, 		 Training Loss: 1.255268Epoch 64 		 Validation Loss: 1.278017, 		 Training Loss: 1.274975Epoch 65 		 Validation Loss: 1.277973, 		 Training Loss: 1.271292Epoch 66 		 Validation Loss: 1.277943, 		 Training Loss: 1.268721Epoch 67 		 Validation Loss: 1.277970, 		 Training Loss: 1.264630Epoch 68 		 Validation Loss: 1.277971, 		 Training Loss: 1.262281Epoch 69 		 Validation Loss: 1.277999, 		 Training Loss: 1.261523Epoch 70 		 Validation Loss: 1.277947, 		 Training Loss: 1.278902Epoch 71 		 Validation Loss: 1.277912, 		 Training Loss: 1.268925Epoch 72 		 Validation Loss: 1.277917, 		 Training Loss: 1.260481Epoch 73 		 Validation Loss: 1.277944, 		 Training Loss: 1.265955Epoch 74 		 Validation Loss: 1.277937, 		 Training Loss: 1.265513Epoch 75 		 Validation Loss: 1.277911, 		 Training Loss: 1.270646Epoch 76 		 Validation Loss: 1.277951, 		 Training Loss: 1.271840Epoch 77 		 Validation Loss: 1.277844, 		 Training Loss: 1.259775Epoch 78 		 Validation Loss: 1.277822, 		 Training Loss: 1.264257Epoch 79 		 Validation Loss: 1.277808, 		 Training Loss: 1.268876Epoch 80 		 Validation Loss: 1.277855, 		 Training Loss: 1.259758Epoch 81 		 Validation Loss: 1.277836, 		 Training Loss: 1.264067Epoch 82 		 Validation Loss: 1.277855, 		 Training Loss: 1.265469Epoch 83 		 Validation Loss: 1.277848, 		 Training Loss: 1.264196Epoch 84 		 Validation Loss: 1.277893, 		 Training Loss: 1.265920Epoch 85 		 Validation Loss: 1.277848, 		 Training Loss: 1.266270Epoch 86 		 Validation Loss: 1.277807, 		 Training Loss: 1.273293Epoch 87 		 Validation Loss: 1.277812, 		 Training Loss: 1.268961Epoch 88 		 Validation Loss: 1.277750, 		 Training Loss: 1.288816Epoch 89 		 Validation Loss: 1.277756, 		 Training Loss: 1.268369Epoch 90 		 Validation Loss: 1.277750, 		 Training Loss: 1.268143Epoch 91 		 Validation Loss: 1.277727, 		 Training Loss: 1.266815Validation Loss Decreased(71.552662--->71.552317) Saving model ...
Epoch 92 		 Validation Loss: 1.277720, 		 Training Loss: 1.263732Epoch 93 		 Validation Loss: 1.277756, 		 Training Loss: 1.257387Epoch 94 		 Validation Loss: 1.277786, 		 Training Loss: 1.266563Epoch 95 		 Validation Loss: 1.277759, 		 Training Loss: 1.273207Epoch 96 		 Validation Loss: 1.277778, 		 Training Loss: 1.263459Epoch 97 		 Validation Loss: 1.277811, 		 Training Loss: 1.271247Epoch 98 		 Validation Loss: 1.277726, 		 Training Loss: 1.276387Epoch 99 		 Validation Loss: 1.277735, 		 Training Loss: 1.264524Epoch 100 		 Validation Loss: 1.277765, 		 Training Loss: 1.267336Epoch 101 		 Validation Loss: 1.277740, 		 Training Loss: 1.259967Epoch 102 		 Validation Loss: 1.277746, 		 Training Loss: 1.273657Epoch 103 		 Validation Loss: 1.277768, 		 Training Loss: 1.269694Validation Loss Decreased(71.552317--->71.552085) Saving model ...
Epoch 104 		 Validation Loss: 1.277716, 		 Training Loss: 1.273549Epoch 105 		 Validation Loss: 1.277723, 		 Training Loss: 1.261703Epoch 106 		 Validation Loss: 1.277783, 		 Training Loss: 1.261311Epoch 107 		 Validation Loss: 1.277741, 		 Training Loss: 1.261881Epoch 108 		 Validation Loss: 1.277773, 		 Training Loss: 1.264914Validation Loss Decreased(71.552085--->71.551982) Saving model ...
Epoch 109 		 Validation Loss: 1.277714, 		 Training Loss: 1.269241Validation Loss Decreased(71.551982--->71.551356) Saving model ...
Epoch 110 		 Validation Loss: 1.277703, 		 Training Loss: 1.268741Validation Loss Decreased(71.551356--->71.550944) Saving model ...
Epoch 111 		 Validation Loss: 1.277695, 		 Training Loss: 1.268167Validation Loss Decreased(71.550944--->71.550733) Saving model ...
Epoch 112 		 Validation Loss: 1.277692, 		 Training Loss: 1.274858Epoch 113 		 Validation Loss: 1.277802, 		 Training Loss: 1.253977Epoch 114 		 Validation Loss: 1.277737, 		 Training Loss: 1.269558Epoch 115 		 Validation Loss: 1.277779, 		 Training Loss: 1.259850Epoch 116 		 Validation Loss: 1.277766, 		 Training Loss: 1.263846Epoch 117 		 Validation Loss: 1.277748, 		 Training Loss: 1.265518Epoch 118 		 Validation Loss: 1.277731, 		 Training Loss: 1.270614Epoch 119 		 Validation Loss: 1.277740, 		 Training Loss: 1.264269Epoch 120 		 Validation Loss: 1.277781, 		 Training Loss: 1.268885Epoch 121 		 Validation Loss: 1.277732, 		 Training Loss: 1.266957Epoch 122 		 Validation Loss: 1.277705, 		 Training Loss: 1.269990Validation Loss Decreased(71.550733--->71.550576) Saving model ...
Epoch 123 		 Validation Loss: 1.277689, 		 Training Loss: 1.265632Epoch 124 		 Validation Loss: 1.277690, 		 Training Loss: 1.260362Validation Loss Decreased(71.550576--->71.549353) Saving model ...
Epoch 125 		 Validation Loss: 1.277667, 		 Training Loss: 1.269213Epoch 126 		 Validation Loss: 1.277736, 		 Training Loss: 1.255190Epoch 127 		 Validation Loss: 1.277671, 		 Training Loss: 1.268617Validation Loss Decreased(71.549353--->71.547564) Saving model ...
Epoch 128 		 Validation Loss: 1.277635, 		 Training Loss: 1.276620Epoch 129 		 Validation Loss: 1.277663, 		 Training Loss: 1.271140Validation Loss Decreased(71.547564--->71.545156) Saving model ...
Epoch 130 		 Validation Loss: 1.277592, 		 Training Loss: 1.273409Validation Loss Decreased(71.545156--->71.543668) Saving model ...
Epoch 131 		 Validation Loss: 1.277565, 		 Training Loss: 1.276466Epoch 132 		 Validation Loss: 1.277577, 		 Training Loss: 1.259249Validation Loss Decreased(71.543668--->71.541643) Saving model ...
Epoch 133 		 Validation Loss: 1.277529, 		 Training Loss: 1.280602Epoch 134 		 Validation Loss: 1.277564, 		 Training Loss: 1.258180Validation Loss Decreased(71.541643--->71.541528) Saving model ...
Epoch 135 		 Validation Loss: 1.277527, 		 Training Loss: 1.273010Validation Loss Decreased(71.541528--->71.539538) Saving model ...
Epoch 136 		 Validation Loss: 1.277492, 		 Training Loss: 1.280198Epoch 137 		 Validation Loss: 1.277514, 		 Training Loss: 1.261285Epoch 138 		 Validation Loss: 1.277510, 		 Training Loss: 1.263400Epoch 139 		 Validation Loss: 1.277498, 		 Training Loss: 1.260360Validation Loss Decreased(71.539538--->71.539537) Saving model ...
Epoch 140 		 Validation Loss: 1.277492, 		 Training Loss: 1.267782Validation Loss Decreased(71.539537--->71.536426) Saving model ...
Epoch 141 		 Validation Loss: 1.277436, 		 Training Loss: 1.277023Validation Loss Decreased(71.536426--->71.536035) Saving model ...
Epoch 142 		 Validation Loss: 1.277429, 		 Training Loss: 1.266620Validation Loss Decreased(71.536035--->71.535297) Saving model ...
Epoch 143 		 Validation Loss: 1.277416, 		 Training Loss: 1.268129Epoch 144 		 Validation Loss: 1.277446, 		 Training Loss: 1.262522Validation Loss Decreased(71.535297--->71.534981) Saving model ...
Epoch 145 		 Validation Loss: 1.277410, 		 Training Loss: 1.274484Epoch 146 		 Validation Loss: 1.277425, 		 Training Loss: 1.262417Epoch 147 		 Validation Loss: 1.277446, 		 Training Loss: 1.261842Epoch 148 		 Validation Loss: 1.277445, 		 Training Loss: 1.259347Epoch 149 		 Validation Loss: 1.277513, 		 Training Loss: 1.263015Epoch 150 		 Validation Loss: 1.277413, 		 Training Loss: 1.271510Epoch 151 		 Validation Loss: 1.277438, 		 Training Loss: 1.261073Epoch 152 		 Validation Loss: 1.277412, 		 Training Loss: 1.273416Epoch 153 		 Validation Loss: 1.277416, 		 Training Loss: 1.271056Epoch 154 		 Validation Loss: 1.277415, 		 Training Loss: 1.264219Validation Loss Decreased(71.534981--->71.534864) Saving model ...
Epoch 155 		 Validation Loss: 1.277408, 		 Training Loss: 1.268418Validation Loss Decreased(71.534864--->71.533956) Saving model ...
Epoch 156 		 Validation Loss: 1.277392, 		 Training Loss: 1.265992Epoch 157 		 Validation Loss: 1.277453, 		 Training Loss: 1.267254Epoch 158 		 Validation Loss: 1.277440, 		 Training Loss: 1.262043Epoch 159 		 Validation Loss: 1.277505, 		 Training Loss: 1.277757Epoch 160 		 Validation Loss: 1.277436, 		 Training Loss: 1.255858Epoch 161 		 Validation Loss: 1.277401, 		 Training Loss: 1.264980Epoch 162 		 Validation Loss: 1.277452, 		 Training Loss: 1.268649Validation Loss Decreased(71.533956--->71.529831) Saving model ...
Epoch 163 		 Validation Loss: 1.277318, 		 Training Loss: 1.282417Epoch 164 		 Validation Loss: 1.277378, 		 Training Loss: 1.270924Epoch 165 		 Validation Loss: 1.277334, 		 Training Loss: 1.286951Validation Loss Decreased(71.529831--->71.527872) Saving model ...
Epoch 166 		 Validation Loss: 1.277283, 		 Training Loss: 1.262484Epoch 167 		 Validation Loss: 1.277339, 		 Training Loss: 1.261294Epoch 168 		 Validation Loss: 1.277327, 		 Training Loss: 1.261029Epoch 169 		 Validation Loss: 1.277293, 		 Training Loss: 1.259264Epoch 170 		 Validation Loss: 1.277340, 		 Training Loss: 1.266011Validation Loss Decreased(71.527872--->71.527745) Saving model ...
Epoch 171 		 Validation Loss: 1.277281, 		 Training Loss: 1.266340Epoch 172 		 Validation Loss: 1.277330, 		 Training Loss: 1.272648Validation Loss Decreased(71.527745--->71.525216) Saving model ...
Epoch 173 		 Validation Loss: 1.277236, 		 Training Loss: 1.268001Epoch 174 		 Validation Loss: 1.277238, 		 Training Loss: 1.265262Validation Loss Decreased(71.525216--->71.524611) Saving model ...
Epoch 175 		 Validation Loss: 1.277225, 		 Training Loss: 1.275920Epoch 176 		 Validation Loss: 1.277279, 		 Training Loss: 1.258023Epoch 177 		 Validation Loss: 1.277290, 		 Training Loss: 1.254417Epoch 178 		 Validation Loss: 1.277276, 		 Training Loss: 1.254975Epoch 179 		 Validation Loss: 1.277260, 		 Training Loss: 1.262607Epoch 180 		 Validation Loss: 1.277238, 		 Training Loss: 1.274418Epoch 181 		 Validation Loss: 1.277321, 		 Training Loss: 1.260430Epoch 182 		 Validation Loss: 1.277331, 		 Training Loss: 1.268831Epoch 183 		 Validation Loss: 1.277260, 		 Training Loss: 1.261359Epoch 184 		 Validation Loss: 1.277265, 		 Training Loss: 1.264577Epoch 185 		 Validation Loss: 1.277316, 		 Training Loss: 1.267961Epoch 186 		 Validation Loss: 1.277264, 		 Training Loss: 1.269541Epoch 187 		 Validation Loss: 1.277244, 		 Training Loss: 1.265430Epoch 188 		 Validation Loss: 1.277280, 		 Training Loss: 1.268690Validation Loss Decreased(71.524611--->71.524597) Saving model ...
Epoch 189 		 Validation Loss: 1.277225, 		 Training Loss: 1.267304Epoch 190 		 Validation Loss: 1.277280, 		 Training Loss: 1.262947Epoch 191 		 Validation Loss: 1.277254, 		 Training Loss: 1.256775Epoch 192 		 Validation Loss: 1.277225, 		 Training Loss: 1.269263Epoch 193 		 Validation Loss: 1.277274, 		 Training Loss: 1.259927Epoch 194 		 Validation Loss: 1.277253, 		 Training Loss: 1.271444Validation Loss Decreased(71.524597--->71.524224) Saving model ...
Epoch 195 		 Validation Loss: 1.277218, 		 Training Loss: 1.257200Validation Loss Decreased(71.524224--->71.522188) Saving model ...
Epoch 196 		 Validation Loss: 1.277182, 		 Training Loss: 1.274579Validation Loss Decreased(71.522188--->71.521349) Saving model ...
Epoch 197 		 Validation Loss: 1.277167, 		 Training Loss: 1.273099Epoch 198 		 Validation Loss: 1.277172, 		 Training Loss: 1.272706Epoch 199 		 Validation Loss: 1.277186, 		 Training Loss: 1.264472Validation Loss Decreased(71.521349--->71.519002) Saving model ...
Epoch 200 		 Validation Loss: 1.277125, 		 Training Loss: 1.269591Epoch 201 		 Validation Loss: 1.277127, 		 Training Loss: 1.259961Epoch 202 		 Validation Loss: 1.277174, 		 Training Loss: 1.259356Epoch 203 		 Validation Loss: 1.277155, 		 Training Loss: 1.272772Validation Loss Decreased(71.519002--->71.517008) Saving model ...
Epoch 204 		 Validation Loss: 1.277089, 		 Training Loss: 1.276020Validation Loss Decreased(71.517008--->71.515944) Saving model ...
Epoch 205 		 Validation Loss: 1.277070, 		 Training Loss: 1.268453Validation Loss Decreased(71.515944--->71.514187) Saving model ...
Epoch 206 		 Validation Loss: 1.277039, 		 Training Loss: 1.270877Epoch 207 		 Validation Loss: 1.277095, 		 Training Loss: 1.267263Epoch 208 		 Validation Loss: 1.277082, 		 Training Loss: 1.255962Epoch 209 		 Validation Loss: 1.277103, 		 Training Loss: 1.268228Epoch 210 		 Validation Loss: 1.277096, 		 Training Loss: 1.266534Epoch 211 		 Validation Loss: 1.277075, 		 Training Loss: 1.266551Epoch 212 		 Validation Loss: 1.277058, 		 Training Loss: 1.262350Epoch 213 		 Validation Loss: 1.277054, 		 Training Loss: 1.273384Epoch 214 		 Validation Loss: 1.277049, 		 Training Loss: 1.263056Validation Loss Decreased(71.514187--->71.513668) Saving model ...
Epoch 215 		 Validation Loss: 1.277030, 		 Training Loss: 1.266506Epoch 216 		 Validation Loss: 1.277123, 		 Training Loss: 1.264946Epoch 217 		 Validation Loss: 1.277103, 		 Training Loss: 1.257875Epoch 218 		 Validation Loss: 1.277122, 		 Training Loss: 1.251645Epoch 219 		 Validation Loss: 1.277165, 		 Training Loss: 1.263089Epoch 220 		 Validation Loss: 1.277131, 		 Training Loss: 1.268810Epoch 221 		 Validation Loss: 1.277095, 		 Training Loss: 1.269032Epoch 222 		 Validation Loss: 1.277099, 		 Training Loss: 1.261443Epoch 223 		 Validation Loss: 1.277131, 		 Training Loss: 1.262690Epoch 224 		 Validation Loss: 1.277117, 		 Training Loss: 1.265917Epoch 225 		 Validation Loss: 1.277181, 		 Training Loss: 1.255934Epoch 226 		 Validation Loss: 1.277126, 		 Training Loss: 1.273983Epoch 227 		 Validation Loss: 1.277142, 		 Training Loss: 1.270628Epoch 228 		 Validation Loss: 1.277104, 		 Training Loss: 1.257685Epoch 229 		 Validation Loss: 1.277088, 		 Training Loss: 1.272919Epoch 230 		 Validation Loss: 1.277082, 		 Training Loss: 1.263167Epoch 231 		 Validation Loss: 1.277100, 		 Training Loss: 1.263992Epoch 232 		 Validation Loss: 1.277064, 		 Training Loss: 1.270917Epoch 233 		 Validation Loss: 1.277100, 		 Training Loss: 1.267306Epoch 234 		 Validation Loss: 1.277113, 		 Training Loss: 1.259050Epoch 235 		 Validation Loss: 1.277076, 		 Training Loss: 1.266956Epoch 236 		 Validation Loss: 1.277075, 		 Training Loss: 1.267985Epoch 237 		 Validation Loss: 1.277089, 		 Training Loss: 1.264006Epoch 238 		 Validation Loss: 1.277092, 		 Training Loss: 1.271665Epoch 239 		 Validation Loss: 1.277042, 		 Training Loss: 1.273596Epoch 240 		 Validation Loss: 1.277114, 		 Training Loss: 1.257011Epoch 241 		 Validation Loss: 1.277046, 		 Training Loss: 1.277658Validation Loss Decreased(71.513668--->71.512525) Saving model ...
Epoch 242 		 Validation Loss: 1.277009, 		 Training Loss: 1.275065Validation Loss Decreased(71.512525--->71.510491) Saving model ...
Epoch 243 		 Validation Loss: 1.276973, 		 Training Loss: 1.277843Validation Loss Decreased(71.510491--->71.508969) Saving model ...
Epoch 244 		 Validation Loss: 1.276946, 		 Training Loss: 1.271246Validation Loss Decreased(71.508969--->71.507963) Saving model ...
Epoch 245 		 Validation Loss: 1.276928, 		 Training Loss: 1.267590Epoch 246 		 Validation Loss: 1.276928, 		 Training Loss: 1.266128Epoch 247 		 Validation Loss: 1.276949, 		 Training Loss: 1.264121Epoch 248 		 Validation Loss: 1.276948, 		 Training Loss: 1.269825Validation Loss Decreased(71.507963--->71.505908) Saving model ...
Epoch 249 		 Validation Loss: 1.276891, 		 Training Loss: 1.266378Epoch 250 		 Validation Loss: 1.276892, 		 Training Loss: 1.268017Validation Loss Decreased(71.505908--->71.505897) Saving model ...
Epoch 251 		 Validation Loss: 1.276891, 		 Training Loss: 1.262643Epoch 252 		 Validation Loss: 1.276896, 		 Training Loss: 1.263949Epoch 253 		 Validation Loss: 1.276949, 		 Training Loss: 1.261594Epoch 254 		 Validation Loss: 1.276900, 		 Training Loss: 1.256324Epoch 255 		 Validation Loss: 1.276900, 		 Training Loss: 1.261905Epoch 256 		 Validation Loss: 1.276949, 		 Training Loss: 1.265289Validation Loss Decreased(71.505897--->71.504973) Saving model ...
Epoch 257 		 Validation Loss: 1.276875, 		 Training Loss: 1.266466Epoch 258 		 Validation Loss: 1.276946, 		 Training Loss: 1.267552Epoch 259 		 Validation Loss: 1.276882, 		 Training Loss: 1.268385Epoch 260 		 Validation Loss: 1.276926, 		 Training Loss: 1.267496Validation Loss Decreased(71.504973--->71.503779) Saving model ...
Epoch 261 		 Validation Loss: 1.276853, 		 Training Loss: 1.271406Validation Loss Decreased(71.503779--->71.503653) Saving model ...
Epoch 262 		 Validation Loss: 1.276851, 		 Training Loss: 1.263897Validation Loss Decreased(71.503653--->71.502996) Saving model ...
Epoch 263 		 Validation Loss: 1.276839, 		 Training Loss: 1.267047Epoch 264 		 Validation Loss: 1.276848, 		 Training Loss: 1.268583Epoch 265 		 Validation Loss: 1.276869, 		 Training Loss: 1.259497Epoch 266 		 Validation Loss: 1.276878, 		 Training Loss: 1.263151Epoch 267 		 Validation Loss: 1.276852, 		 Training Loss: 1.261170Epoch 268 		 Validation Loss: 1.276878, 		 Training Loss: 1.253439Validation Loss Decreased(71.502996--->71.502814) Saving model ...
Epoch 269 		 Validation Loss: 1.276836, 		 Training Loss: 1.282047Validation Loss Decreased(71.502814--->71.502781) Saving model ...
Epoch 270 		 Validation Loss: 1.276835, 		 Training Loss: 1.269795Validation Loss Decreased(71.502781--->71.502504) Saving model ...
Epoch 271 		 Validation Loss: 1.276830, 		 Training Loss: 1.262555Validation Loss Decreased(71.502504--->71.501609) Saving model ...
Epoch 272 		 Validation Loss: 1.276814, 		 Training Loss: 1.267305Validation Loss Decreased(71.501609--->71.498660) Saving model ...
Epoch 273 		 Validation Loss: 1.276762, 		 Training Loss: 1.275714Epoch 274 		 Validation Loss: 1.276800, 		 Training Loss: 1.257963Epoch 275 		 Validation Loss: 1.276793, 		 Training Loss: 1.262856Epoch 276 		 Validation Loss: 1.276833, 		 Training Loss: 1.271000Epoch 277 		 Validation Loss: 1.276815, 		 Training Loss: 1.255720Epoch 278 		 Validation Loss: 1.276800, 		 Training Loss: 1.270847Epoch 279 		 Validation Loss: 1.276819, 		 Training Loss: 1.260103Epoch 280 		 Validation Loss: 1.276839, 		 Training Loss: 1.261884Epoch 281 		 Validation Loss: 1.276818, 		 Training Loss: 1.267481Epoch 282 		 Validation Loss: 1.276778, 		 Training Loss: 1.267391Validation Loss Decreased(71.498660--->71.497091) Saving model ...
Epoch 283 		 Validation Loss: 1.276734, 		 Training Loss: 1.289036Epoch 284 		 Validation Loss: 1.276751, 		 Training Loss: 1.257741Validation Loss Decreased(71.497091--->71.496365) Saving model ...
Epoch 285 		 Validation Loss: 1.276721, 		 Training Loss: 1.270552Epoch 286 		 Validation Loss: 1.276771, 		 Training Loss: 1.255409Epoch 287 		 Validation Loss: 1.276772, 		 Training Loss: 1.264269Epoch 288 		 Validation Loss: 1.276756, 		 Training Loss: 1.271312Epoch 289 		 Validation Loss: 1.276797, 		 Training Loss: 1.259094Epoch 290 		 Validation Loss: 1.276819, 		 Training Loss: 1.258468Epoch 291 		 Validation Loss: 1.276798, 		 Training Loss: 1.263134Epoch 292 		 Validation Loss: 1.276775, 		 Training Loss: 1.276211Epoch 293 		 Validation Loss: 1.276740, 		 Training Loss: 1.264108Validation Loss Decreased(71.496365--->71.495540) Saving model ...
Epoch 294 		 Validation Loss: 1.276706, 		 Training Loss: 1.271634Validation Loss Decreased(71.495540--->71.495093) Saving model ...
Epoch 295 		 Validation Loss: 1.276698, 		 Training Loss: 1.264959Epoch 296 		 Validation Loss: 1.276740, 		 Training Loss: 1.266925Validation Loss Decreased(71.495093--->71.495057) Saving model ...
Epoch 297 		 Validation Loss: 1.276697, 		 Training Loss: 1.265187Epoch 298 		 Validation Loss: 1.276706, 		 Training Loss: 1.257394Epoch 299 		 Validation Loss: 1.276791, 		 Training Loss: 1.258577Epoch 300 		 Validation Loss: 1.276721, 		 Training Loss: 1.273025Epoch 301 		 Validation Loss: 1.276765, 		 Training Loss: 1.265956Validation Loss Decreased(71.495057--->71.491287) Saving model ...
Epoch 302 		 Validation Loss: 1.276630, 		 Training Loss: 1.276263Epoch 303 		 Validation Loss: 1.276647, 		 Training Loss: 1.260958Epoch 304 		 Validation Loss: 1.276663, 		 Training Loss: 1.264375Epoch 305 		 Validation Loss: 1.276709, 		 Training Loss: 1.260004Epoch 306 		 Validation Loss: 1.276679, 		 Training Loss: 1.265787Epoch 307 		 Validation Loss: 1.276742, 		 Training Loss: 1.264451Epoch 308 		 Validation Loss: 1.276750, 		 Training Loss: 1.265511Epoch 309 		 Validation Loss: 1.276716, 		 Training Loss: 1.259522Epoch 310 		 Validation Loss: 1.276743, 		 Training Loss: 1.268792Epoch 311 		 Validation Loss: 1.276684, 		 Training Loss: 1.270744Epoch 312 		 Validation Loss: 1.276647, 		 Training Loss: 1.276173Epoch 313 		 Validation Loss: 1.276665, 		 Training Loss: 1.268748Epoch 314 		 Validation Loss: 1.276640, 		 Training Loss: 1.260078Epoch 315 		 Validation Loss: 1.276650, 		 Training Loss: 1.263399Epoch 316 		 Validation Loss: 1.276654, 		 Training Loss: 1.263860Epoch 317 		 Validation Loss: 1.276664, 		 Training Loss: 1.261999Validation Loss Decreased(71.491287--->71.489768) Saving model ...
Epoch 318 		 Validation Loss: 1.276603, 		 Training Loss: 1.278004Epoch 319 		 Validation Loss: 1.276620, 		 Training Loss: 1.272017Validation Loss Decreased(71.489768--->71.489509) Saving model ...
Epoch 320 		 Validation Loss: 1.276598, 		 Training Loss: 1.268353Epoch 321 		 Validation Loss: 1.276623, 		 Training Loss: 1.256668Epoch 322 		 Validation Loss: 1.276631, 		 Training Loss: 1.267790Validation Loss Decreased(71.489509--->71.489489) Saving model ...
Epoch 323 		 Validation Loss: 1.276598, 		 Training Loss: 1.268138Epoch 324 		 Validation Loss: 1.276631, 		 Training Loss: 1.268662Epoch 325 		 Validation Loss: 1.276607, 		 Training Loss: 1.265722Validation Loss Decreased(71.489489--->71.488967) Saving model ...
Epoch 326 		 Validation Loss: 1.276589, 		 Training Loss: 1.270920Validation Loss Decreased(71.488967--->71.488759) Saving model ...
Epoch 327 		 Validation Loss: 1.276585, 		 Training Loss: 1.262276Validation Loss Decreased(71.488759--->71.487824) Saving model ...
Epoch 328 		 Validation Loss: 1.276568, 		 Training Loss: 1.268005Epoch 329 		 Validation Loss: 1.276602, 		 Training Loss: 1.257033Epoch 330 		 Validation Loss: 1.276577, 		 Training Loss: 1.260527Epoch 331 		 Validation Loss: 1.276583, 		 Training Loss: 1.260548Epoch 332 		 Validation Loss: 1.276577, 		 Training Loss: 1.268048Epoch 333 		 Validation Loss: 1.276609, 		 Training Loss: 1.259857Epoch 334 		 Validation Loss: 1.276602, 		 Training Loss: 1.257572Epoch 335 		 Validation Loss: 1.276622, 		 Training Loss: 1.257561Epoch 336 		 Validation Loss: 1.276650, 		 Training Loss: 1.257924Epoch 337 		 Validation Loss: 1.276661, 		 Training Loss: 1.260830Epoch 338 		 Validation Loss: 1.276653, 		 Training Loss: 1.266341Epoch 339 		 Validation Loss: 1.276614, 		 Training Loss: 1.270948Epoch 340 		 Validation Loss: 1.276580, 		 Training Loss: 1.278356Validation Loss Decreased(71.487824--->71.487129) Saving model ...
Epoch 341 		 Validation Loss: 1.276556, 		 Training Loss: 1.261962Epoch 342 		 Validation Loss: 1.276634, 		 Training Loss: 1.275031Epoch 343 		 Validation Loss: 1.276579, 		 Training Loss: 1.264118Validation Loss Decreased(71.487129--->71.486993) Saving model ...
Epoch 344 		 Validation Loss: 1.276553, 		 Training Loss: 1.264276Epoch 345 		 Validation Loss: 1.276621, 		 Training Loss: 1.267862Validation Loss Decreased(71.486993--->71.485547) Saving model ...
Epoch 346 		 Validation Loss: 1.276528, 		 Training Loss: 1.268809Epoch 347 		 Validation Loss: 1.276552, 		 Training Loss: 1.271766Validation Loss Decreased(71.485547--->71.485269) Saving model ...
Epoch 348 		 Validation Loss: 1.276523, 		 Training Loss: 1.257174Validation Loss Decreased(71.485269--->71.483983) Saving model ...
Epoch 349 		 Validation Loss: 1.276500, 		 Training Loss: 1.277980Epoch 350 		 Validation Loss: 1.276606, 		 Training Loss: 1.264066Validation Loss Decreased(71.483983--->71.483941) Saving model ...
Epoch 351 		 Validation Loss: 1.276499, 		 Training Loss: 1.261235Epoch 352 		 Validation Loss: 1.276509, 		 Training Loss: 1.267479Epoch 353 		 Validation Loss: 1.276530, 		 Training Loss: 1.265171Epoch 354 		 Validation Loss: 1.276554, 		 Training Loss: 1.252045Epoch 355 		 Validation Loss: 1.276589, 		 Training Loss: 1.262771Epoch 356 		 Validation Loss: 1.276579, 		 Training Loss: 1.257519Epoch 357 		 Validation Loss: 1.276648, 		 Training Loss: 1.262589Epoch 358 		 Validation Loss: 1.276537, 		 Training Loss: 1.268804Epoch 359 		 Validation Loss: 1.276530, 		 Training Loss: 1.264390Epoch 360 		 Validation Loss: 1.276527, 		 Training Loss: 1.262047Epoch 361 		 Validation Loss: 1.276533, 		 Training Loss: 1.265966Epoch 362 		 Validation Loss: 1.276583, 		 Training Loss: 1.271921Epoch 363 		 Validation Loss: 1.276510, 		 Training Loss: 1.268109Validation Loss Decreased(71.483941--->71.481614) Saving model ...
Epoch 364 		 Validation Loss: 1.276457, 		 Training Loss: 1.255003Epoch 365 		 Validation Loss: 1.276493, 		 Training Loss: 1.265170Epoch 366 		 Validation Loss: 1.276466, 		 Training Loss: 1.267650Validation Loss Decreased(71.481614--->71.480995) Saving model ...
Epoch 367 		 Validation Loss: 1.276446, 		 Training Loss: 1.275990Epoch 368 		 Validation Loss: 1.276451, 		 Training Loss: 1.263746Epoch 369 		 Validation Loss: 1.276497, 		 Training Loss: 1.273004Validation Loss Decreased(71.480995--->71.480159) Saving model ...
Epoch 370 		 Validation Loss: 1.276431, 		 Training Loss: 1.259630Epoch 371 		 Validation Loss: 1.276454, 		 Training Loss: 1.263257Epoch 372 		 Validation Loss: 1.276477, 		 Training Loss: 1.261304Epoch 373 		 Validation Loss: 1.276517, 		 Training Loss: 1.262347Epoch 374 		 Validation Loss: 1.276435, 		 Training Loss: 1.273013Epoch 375 		 Validation Loss: 1.276463, 		 Training Loss: 1.264144Epoch 376 		 Validation Loss: 1.276456, 		 Training Loss: 1.263234Epoch 377 		 Validation Loss: 1.276481, 		 Training Loss: 1.264649Epoch 378 		 Validation Loss: 1.276473, 		 Training Loss: 1.273051Validation Loss Decreased(71.480159--->71.478634) Saving model ...
Epoch 379 		 Validation Loss: 1.276404, 		 Training Loss: 1.268529Epoch 380 		 Validation Loss: 1.276445, 		 Training Loss: 1.257829Epoch 381 		 Validation Loss: 1.276461, 		 Training Loss: 1.265544Epoch 382 		 Validation Loss: 1.276481, 		 Training Loss: 1.267808Epoch 383 		 Validation Loss: 1.276497, 		 Training Loss: 1.263246Validation Loss Decreased(71.478634--->71.477516) Saving model ...
Epoch 384 		 Validation Loss: 1.276384, 		 Training Loss: 1.272218Validation Loss Decreased(71.477516--->71.477334) Saving model ...
Epoch 385 		 Validation Loss: 1.276381, 		 Training Loss: 1.269573Validation Loss Decreased(71.477334--->71.476784) Saving model ...
Epoch 386 		 Validation Loss: 1.276371, 		 Training Loss: 1.269271Epoch 387 		 Validation Loss: 1.276417, 		 Training Loss: 1.266129Epoch 388 		 Validation Loss: 1.276397, 		 Training Loss: 1.262769Epoch 389 		 Validation Loss: 1.276415, 		 Training Loss: 1.261796Epoch 390 		 Validation Loss: 1.276412, 		 Training Loss: 1.269860Validation Loss Decreased(71.476784--->71.476680) Saving model ...
Epoch 391 		 Validation Loss: 1.276369, 		 Training Loss: 1.261049Validation Loss Decreased(71.476680--->71.476131) Saving model ...
Epoch 392 		 Validation Loss: 1.276359, 		 Training Loss: 1.267661Validation Loss Decreased(71.476131--->71.474935) Saving model ...
Epoch 393 		 Validation Loss: 1.276338, 		 Training Loss: 1.264923Epoch 394 		 Validation Loss: 1.276393, 		 Training Loss: 1.261238Epoch 395 		 Validation Loss: 1.276347, 		 Training Loss: 1.269076Epoch 396 		 Validation Loss: 1.276408, 		 Training Loss: 1.258737Epoch 397 		 Validation Loss: 1.276442, 		 Training Loss: 1.255178Epoch 398 		 Validation Loss: 1.276415, 		 Training Loss: 1.257987Epoch 399 		 Validation Loss: 1.276407, 		 Training Loss: 1.261977Epoch 400 		 Validation Loss: 1.276420, 		 Training Loss: 1.261891Epoch 401 		 Validation Loss: 1.276423, 		 Training Loss: 1.261235Epoch 402 		 Validation Loss: 1.276444, 		 Training Loss: 1.257699Epoch 403 		 Validation Loss: 1.276410, 		 Training Loss: 1.261841Epoch 404 		 Validation Loss: 1.276397, 		 Training Loss: 1.264259Epoch 405 		 Validation Loss: 1.276418, 		 Training Loss: 1.260003Epoch 406 		 Validation Loss: 1.276425, 		 Training Loss: 1.259264Epoch 407 		 Validation Loss: 1.276434, 		 Training Loss: 1.267623Epoch 408 		 Validation Loss: 1.276398, 		 Training Loss: 1.270756Epoch 409 		 Validation Loss: 1.276385, 		 Training Loss: 1.267266Epoch 410 		 Validation Loss: 1.276368, 		 Training Loss: 1.270701Epoch 411 		 Validation Loss: 1.276364, 		 Training Loss: 1.268725Epoch 412 		 Validation Loss: 1.276360, 		 Training Loss: 1.263545Epoch 413 		 Validation Loss: 1.276361, 		 Training Loss: 1.263770Epoch 414 		 Validation Loss: 1.276366, 		 Training Loss: 1.261839Epoch 415 		 Validation Loss: 1.276359, 		 Training Loss: 1.261655Epoch 416 		 Validation Loss: 1.276351, 		 Training Loss: 1.271154Epoch 417 		 Validation Loss: 1.276365, 		 Training Loss: 1.263708Epoch 418 		 Validation Loss: 1.276368, 		 Training Loss: 1.259330Epoch 419 		 Validation Loss: 1.276381, 		 Training Loss: 1.256644Validation Loss Decreased(71.474935--->71.473803) Saving model ...
Epoch 420 		 Validation Loss: 1.276318, 		 Training Loss: 1.270298Epoch 421 		 Validation Loss: 1.276353, 		 Training Loss: 1.262664Epoch 422 		 Validation Loss: 1.276334, 		 Training Loss: 1.276055Validation Loss Decreased(71.473803--->71.473598) Saving model ...
Epoch 423 		 Validation Loss: 1.276314, 		 Training Loss: 1.265371Validation Loss Decreased(71.473598--->71.472839) Saving model ...
Epoch 424 		 Validation Loss: 1.276301, 		 Training Loss: 1.264523Epoch 425 		 Validation Loss: 1.276305, 		 Training Loss: 1.261858Epoch 426 		 Validation Loss: 1.276308, 		 Training Loss: 1.260317Validation Loss Decreased(71.472839--->71.472001) Saving model ...
Epoch 427 		 Validation Loss: 1.276286, 		 Training Loss: 1.269692Validation Loss Decreased(71.472001--->71.471635) Saving model ...
Epoch 428 		 Validation Loss: 1.276279, 		 Training Loss: 1.269554Validation Loss Decreased(71.471635--->71.471087) Saving model ...
Epoch 429 		 Validation Loss: 1.276269, 		 Training Loss: 1.266708Validation Loss Decreased(71.471087--->71.469088) Saving model ...
Epoch 430 		 Validation Loss: 1.276234, 		 Training Loss: 1.270275Epoch 431 		 Validation Loss: 1.276243, 		 Training Loss: 1.261066Epoch 432 		 Validation Loss: 1.276272, 		 Training Loss: 1.254815Epoch 433 		 Validation Loss: 1.276259, 		 Training Loss: 1.267223Epoch 434 		 Validation Loss: 1.276285, 		 Training Loss: 1.254868Epoch 435 		 Validation Loss: 1.276258, 		 Training Loss: 1.272961Epoch 436 		 Validation Loss: 1.276237, 		 Training Loss: 1.262105Epoch 437 		 Validation Loss: 1.276264, 		 Training Loss: 1.264157Epoch 438 		 Validation Loss: 1.276298, 		 Training Loss: 1.269017Epoch 439 		 Validation Loss: 1.276278, 		 Training Loss: 1.264358Epoch 440 		 Validation Loss: 1.276296, 		 Training Loss: 1.257251Epoch 441 		 Validation Loss: 1.276320, 		 Training Loss: 1.268384Epoch 442 		 Validation Loss: 1.276312, 		 Training Loss: 1.262289Epoch 443 		 Validation Loss: 1.276303, 		 Training Loss: 1.261571Epoch 444 		 Validation Loss: 1.276316, 		 Training Loss: 1.256370Epoch 445 		 Validation Loss: 1.276336, 		 Training Loss: 1.265793Epoch 446 		 Validation Loss: 1.276347, 		 Training Loss: 1.262545Epoch 447 		 Validation Loss: 1.276348, 		 Training Loss: 1.253522Epoch 448 		 Validation Loss: 1.276310, 		 Training Loss: 1.273333Epoch 449 		 Validation Loss: 1.276320, 		 Training Loss: 1.262754Epoch 450 		 Validation Loss: 1.276342, 		 Training Loss: 1.264664Epoch 451 		 Validation Loss: 1.276370, 		 Training Loss: 1.253504Epoch 452 		 Validation Loss: 1.276379, 		 Training Loss: 1.256098Epoch 453 		 Validation Loss: 1.276388, 		 Training Loss: 1.259459Epoch 454 		 Validation Loss: 1.276392, 		 Training Loss: 1.266102Epoch 455 		 Validation Loss: 1.276304, 		 Training Loss: 1.273846Epoch 456 		 Validation Loss: 1.276329, 		 Training Loss: 1.266347Epoch 457 		 Validation Loss: 1.276342, 		 Training Loss: 1.260087Epoch 458 		 Validation Loss: 1.276323, 		 Training Loss: 1.270331Epoch 459 		 Validation Loss: 1.276285, 		 Training Loss: 1.264531Epoch 460 		 Validation Loss: 1.276293, 		 Training Loss: 1.267003Epoch 461 		 Validation Loss: 1.276243, 		 Training Loss: 1.267745Epoch 462 		 Validation Loss: 1.276335, 		 Training Loss: 1.264921Epoch 463 		 Validation Loss: 1.276260, 		 Training Loss: 1.260960Epoch 464 		 Validation Loss: 1.276246, 		 Training Loss: 1.263275Epoch 465 		 Validation Loss: 1.276251, 		 Training Loss: 1.263200Epoch 466 		 Validation Loss: 1.276238, 		 Training Loss: 1.270086Epoch 467 		 Validation Loss: 1.276235, 		 Training Loss: 1.264627Validation Loss Decreased(71.469088--->71.468552) Saving model ...
Epoch 468 		 Validation Loss: 1.276224, 		 Training Loss: 1.264129Validation Loss Decreased(71.468552--->71.467542) Saving model ...
Epoch 469 		 Validation Loss: 1.276206, 		 Training Loss: 1.267467Epoch 470 		 Validation Loss: 1.276226, 		 Training Loss: 1.253395Epoch 471 		 Validation Loss: 1.276234, 		 Training Loss: 1.262447Epoch 472 		 Validation Loss: 1.276264, 		 Training Loss: 1.260623Epoch 473 		 Validation Loss: 1.276231, 		 Training Loss: 1.275394Epoch 474 		 Validation Loss: 1.276219, 		 Training Loss: 1.259946Epoch 475 		 Validation Loss: 1.276254, 		 Training Loss: 1.254586Epoch 476 		 Validation Loss: 1.276252, 		 Training Loss: 1.260795Epoch 477 		 Validation Loss: 1.276276, 		 Training Loss: 1.259888Epoch 478 		 Validation Loss: 1.276263, 		 Training Loss: 1.260927Epoch 479 		 Validation Loss: 1.276293, 		 Training Loss: 1.259510Epoch 480 		 Validation Loss: 1.276299, 		 Training Loss: 1.259805Epoch 481 		 Validation Loss: 1.276296, 		 Training Loss: 1.264150Epoch 482 		 Validation Loss: 1.276303, 		 Training Loss: 1.259692Epoch 483 		 Validation Loss: 1.276222, 		 Training Loss: 1.284805Epoch 484 		 Validation Loss: 1.276237, 		 Training Loss: 1.260838Epoch 485 		 Validation Loss: 1.276222, 		 Training Loss: 1.264812Epoch 486 		 Validation Loss: 1.276212, 		 Training Loss: 1.261367Validation Loss Decreased(71.467542--->71.466982) Saving model ...
Epoch 487 		 Validation Loss: 1.276196, 		 Training Loss: 1.269300Validation Loss Decreased(71.466982--->71.466675) Saving model ...
Epoch 488 		 Validation Loss: 1.276191, 		 Training Loss: 1.253829Validation Loss Decreased(71.466675--->71.465518) Saving model ...
Epoch 489 		 Validation Loss: 1.276170, 		 Training Loss: 1.270943Validation Loss Decreased(71.465518--->71.464439) Saving model ...
Epoch 490 		 Validation Loss: 1.276151, 		 Training Loss: 1.263187Validation Loss Decreased(71.464439--->71.462387) Saving model ...
Epoch 491 		 Validation Loss: 1.276114, 		 Training Loss: 1.261712Validation Loss Decreased(71.462387--->71.462383) Saving model ...
Epoch 492 		 Validation Loss: 1.276114, 		 Training Loss: 1.270640Validation Loss Decreased(71.462383--->71.461082) Saving model ...
Epoch 493 		 Validation Loss: 1.276091, 		 Training Loss: 1.263727Epoch 494 		 Validation Loss: 1.276141, 		 Training Loss: 1.268855Validation Loss Decreased(71.461082--->71.457987) Saving model ...
Epoch 495 		 Validation Loss: 1.276035, 		 Training Loss: 1.273734Epoch 496 		 Validation Loss: 1.276038, 		 Training Loss: 1.265670Epoch 497 		 Validation Loss: 1.276056, 		 Training Loss: 1.274361Epoch 498 		 Validation Loss: 1.276039, 		 Training Loss: 1.261446Epoch 499 		 Validation Loss: 1.276050, 		 Training Loss: 1.273162Validation Loss Decreased(71.457987--->71.457038) Saving model ...
Epoch 500 		 Validation Loss: 1.276019, 		 Training Loss: 1.269248Validation Loss Decreased(71.457038--->71.456271) Saving model ...
Epoch 501 		 Validation Loss: 1.276005, 		 Training Loss: 1.261802Epoch 502 		 Validation Loss: 1.276075, 		 Training Loss: 1.255072Validation Loss Decreased(71.456271--->71.455315) Saving model ...
Epoch 503 		 Validation Loss: 1.275988, 		 Training Loss: 1.275248Validation Loss Decreased(71.455315--->71.455045) Saving model ...
Epoch 504 		 Validation Loss: 1.275983, 		 Training Loss: 1.264326Validation Loss Decreased(71.455045--->71.454628) Saving model ...
Epoch 505 		 Validation Loss: 1.275976, 		 Training Loss: 1.269259Validation Loss Decreased(71.454628--->71.454592) Saving model ...
Epoch 506 		 Validation Loss: 1.275975, 		 Training Loss: 1.253854Epoch 507 		 Validation Loss: 1.276016, 		 Training Loss: 1.260590Validation Loss Decreased(71.454592--->71.454131) Saving model ...
Epoch 508 		 Validation Loss: 1.275967, 		 Training Loss: 1.271366Epoch 509 		 Validation Loss: 1.275988, 		 Training Loss: 1.257514Epoch 510 		 Validation Loss: 1.275975, 		 Training Loss: 1.261072Validation Loss Decreased(71.454131--->71.453699) Saving model ...
Epoch 511 		 Validation Loss: 1.275959, 		 Training Loss: 1.258955Validation Loss Decreased(71.453699--->71.452850) Saving model ...
Epoch 512 		 Validation Loss: 1.275944, 		 Training Loss: 1.261808Epoch 513 		 Validation Loss: 1.275977, 		 Training Loss: 1.259372Validation Loss Decreased(71.452850--->71.451240) Saving model ...
Epoch 514 		 Validation Loss: 1.275915, 		 Training Loss: 1.269921Validation Loss Decreased(71.451240--->71.450674) Saving model ...
Epoch 515 		 Validation Loss: 1.275905, 		 Training Loss: 1.268171Epoch 516 		 Validation Loss: 1.275930, 		 Training Loss: 1.257931Epoch 517 		 Validation Loss: 1.275963, 		 Training Loss: 1.260436Epoch 518 		 Validation Loss: 1.276017, 		 Training Loss: 1.253110Epoch 519 		 Validation Loss: 1.276006, 		 Training Loss: 1.258877Epoch 520 		 Validation Loss: 1.275955, 		 Training Loss: 1.267146Epoch 521 		 Validation Loss: 1.275989, 		 Training Loss: 1.266560Epoch 522 		 Validation Loss: 1.275925, 		 Training Loss: 1.265056Epoch 523 		 Validation Loss: 1.275953, 		 Training Loss: 1.254258Epoch 524 		 Validation Loss: 1.275929, 		 Training Loss: 1.270530Epoch 525 		 Validation Loss: 1.275951, 		 Training Loss: 1.267773Epoch 526 		 Validation Loss: 1.275942, 		 Training Loss: 1.257304Epoch 527 		 Validation Loss: 1.276020, 		 Training Loss: 1.256516Epoch 528 		 Validation Loss: 1.275920, 		 Training Loss: 1.268625Epoch 529 		 Validation Loss: 1.275928, 		 Training Loss: 1.261958Epoch 530 		 Validation Loss: 1.275942, 		 Training Loss: 1.261145Epoch 531 		 Validation Loss: 1.275958, 		 Training Loss: 1.265601Epoch 532 		 Validation Loss: 1.275908, 		 Training Loss: 1.273580Validation Loss Decreased(71.450674--->71.450019) Saving model ...
Epoch 533 		 Validation Loss: 1.275893, 		 Training Loss: 1.259318Epoch 534 		 Validation Loss: 1.275921, 		 Training Loss: 1.258898Validation Loss Decreased(71.450019--->71.449483) Saving model ...
Epoch 535 		 Validation Loss: 1.275884, 		 Training Loss: 1.268410Epoch 536 		 Validation Loss: 1.275903, 		 Training Loss: 1.258816Epoch 537 		 Validation Loss: 1.275884, 		 Training Loss: 1.266360Validation Loss Decreased(71.449483--->71.448206) Saving model ...
Epoch 538 		 Validation Loss: 1.275861, 		 Training Loss: 1.269423Validation Loss Decreased(71.448206--->71.445688) Saving model ...
Epoch 539 		 Validation Loss: 1.275816, 		 Training Loss: 1.261444Epoch 540 		 Validation Loss: 1.275896, 		 Training Loss: 1.256248Epoch 541 		 Validation Loss: 1.275828, 		 Training Loss: 1.256316Epoch 542 		 Validation Loss: 1.275857, 		 Training Loss: 1.258013Epoch 543 		 Validation Loss: 1.275817, 		 Training Loss: 1.279814Validation Loss Decreased(71.445688--->71.444341) Saving model ...
Epoch 544 		 Validation Loss: 1.275792, 		 Training Loss: 1.271021Validation Loss Decreased(71.444341--->71.444181) Saving model ...
Epoch 545 		 Validation Loss: 1.275789, 		 Training Loss: 1.259066Epoch 546 		 Validation Loss: 1.275819, 		 Training Loss: 1.256244Epoch 547 		 Validation Loss: 1.275828, 		 Training Loss: 1.266117Epoch 548 		 Validation Loss: 1.275793, 		 Training Loss: 1.267264Validation Loss Decreased(71.444181--->71.443883) Saving model ...
Epoch 549 		 Validation Loss: 1.275784, 		 Training Loss: 1.261361Epoch 550 		 Validation Loss: 1.275798, 		 Training Loss: 1.264091Epoch 551 		 Validation Loss: 1.275811, 		 Training Loss: 1.252262Epoch 552 		 Validation Loss: 1.275789, 		 Training Loss: 1.260086Validation Loss Decreased(71.443883--->71.443608) Saving model ...
Epoch 553 		 Validation Loss: 1.275779, 		 Training Loss: 1.263370Validation Loss Decreased(71.443608--->71.442994) Saving model ...
Epoch 554 		 Validation Loss: 1.275768, 		 Training Loss: 1.260516Epoch 555 		 Validation Loss: 1.275813, 		 Training Loss: 1.255957Epoch 556 		 Validation Loss: 1.275810, 		 Training Loss: 1.256676Validation Loss Decreased(71.442994--->71.441605) Saving model ...
Epoch 557 		 Validation Loss: 1.275743, 		 Training Loss: 1.270741Epoch 558 		 Validation Loss: 1.275748, 		 Training Loss: 1.262065Validation Loss Decreased(71.441605--->71.441565) Saving model ...
Epoch 559 		 Validation Loss: 1.275742, 		 Training Loss: 1.272670Epoch 560 		 Validation Loss: 1.275769, 		 Training Loss: 1.260180Validation Loss Decreased(71.441565--->71.440200) Saving model ...
Epoch 561 		 Validation Loss: 1.275718, 		 Training Loss: 1.268083Validation Loss Decreased(71.440200--->71.438577) Saving model ...
Epoch 562 		 Validation Loss: 1.275689, 		 Training Loss: 1.267010Epoch 563 		 Validation Loss: 1.275729, 		 Training Loss: 1.265899Validation Loss Decreased(71.438577--->71.438176) Saving model ...
Epoch 564 		 Validation Loss: 1.275682, 		 Training Loss: 1.265981Epoch 565 		 Validation Loss: 1.275696, 		 Training Loss: 1.266853Epoch 566 		 Validation Loss: 1.275685, 		 Training Loss: 1.262806Epoch 567 		 Validation Loss: 1.275685, 		 Training Loss: 1.261676Validation Loss Decreased(71.438176--->71.437208) Saving model ...
Epoch 568 		 Validation Loss: 1.275664, 		 Training Loss: 1.262601Epoch 569 		 Validation Loss: 1.275681, 		 Training Loss: 1.272604Epoch 570 		 Validation Loss: 1.275725, 		 Training Loss: 1.250340Validation Loss Decreased(71.437208--->71.436720) Saving model ...
Epoch 571 		 Validation Loss: 1.275656, 		 Training Loss: 1.278041Epoch 572 		 Validation Loss: 1.275657, 		 Training Loss: 1.267940Epoch 573 		 Validation Loss: 1.275705, 		 Training Loss: 1.257354Epoch 574 		 Validation Loss: 1.275706, 		 Training Loss: 1.265740Epoch 575 		 Validation Loss: 1.275714, 		 Training Loss: 1.265508Epoch 576 		 Validation Loss: 1.275660, 		 Training Loss: 1.264970Validation Loss Decreased(71.436720--->71.436218) Saving model ...
Epoch 577 		 Validation Loss: 1.275647, 		 Training Loss: 1.261918Epoch 578 		 Validation Loss: 1.275661, 		 Training Loss: 1.264835Validation Loss Decreased(71.436218--->71.432566) Saving model ...
Epoch 579 		 Validation Loss: 1.275582, 		 Training Loss: 1.286886Validation Loss Decreased(71.432566--->71.432265) Saving model ...
Epoch 580 		 Validation Loss: 1.275576, 		 Training Loss: 1.256011Epoch 581 		 Validation Loss: 1.275600, 		 Training Loss: 1.273095Epoch 582 		 Validation Loss: 1.275592, 		 Training Loss: 1.262933Epoch 583 		 Validation Loss: 1.275631, 		 Training Loss: 1.252139Epoch 584 		 Validation Loss: 1.275615, 		 Training Loss: 1.263457Epoch 585 		 Validation Loss: 1.275586, 		 Training Loss: 1.273014Epoch 586 		 Validation Loss: 1.275601, 		 Training Loss: 1.259598Epoch 587 		 Validation Loss: 1.275578, 		 Training Loss: 1.262976Validation Loss Decreased(71.432265--->71.431821) Saving model ...
Epoch 588 		 Validation Loss: 1.275568, 		 Training Loss: 1.270016Epoch 589 		 Validation Loss: 1.275572, 		 Training Loss: 1.263126Epoch 590 		 Validation Loss: 1.275570, 		 Training Loss: 1.273714Validation Loss Decreased(71.431821--->71.430706) Saving model ...
Epoch 591 		 Validation Loss: 1.275548, 		 Training Loss: 1.260752Epoch 592 		 Validation Loss: 1.275586, 		 Training Loss: 1.262483Epoch 593 		 Validation Loss: 1.275557, 		 Training Loss: 1.264481Epoch 594 		 Validation Loss: 1.275560, 		 Training Loss: 1.265247Validation Loss Decreased(71.430706--->71.429476) Saving model ...
Epoch 595 		 Validation Loss: 1.275526, 		 Training Loss: 1.268271Epoch 596 		 Validation Loss: 1.275536, 		 Training Loss: 1.259703Epoch 597 		 Validation Loss: 1.275530, 		 Training Loss: 1.261038Validation Loss Decreased(71.429476--->71.429336) Saving model ...
Epoch 598 		 Validation Loss: 1.275524, 		 Training Loss: 1.261998Validation Loss Decreased(71.429336--->71.428970) Saving model ...
Epoch 599 		 Validation Loss: 1.275517, 		 Training Loss: 1.273092Epoch 600 		 Validation Loss: 1.275520, 		 Training Loss: 1.261115Epoch 601 		 Validation Loss: 1.275578, 		 Training Loss: 1.267999Epoch 602 		 Validation Loss: 1.275545, 		 Training Loss: 1.262893Epoch 603 		 Validation Loss: 1.275518, 		 Training Loss: 1.261988Epoch 604 		 Validation Loss: 1.275558, 		 Training Loss: 1.251574Epoch 605 		 Validation Loss: 1.275546, 		 Training Loss: 1.263263Epoch 606 		 Validation Loss: 1.275551, 		 Training Loss: 1.252211Epoch 607 		 Validation Loss: 1.275608, 		 Training Loss: 1.258874Epoch 608 		 Validation Loss: 1.275603, 		 Training Loss: 1.257773Epoch 609 		 Validation Loss: 1.275542, 		 Training Loss: 1.270207Epoch 610 		 Validation Loss: 1.275523, 		 Training Loss: 1.264214Validation Loss Decreased(71.428970--->71.428929) Saving model ...
Epoch 611 		 Validation Loss: 1.275517, 		 Training Loss: 1.261455Epoch 612 		 Validation Loss: 1.275540, 		 Training Loss: 1.267506Epoch 613 		 Validation Loss: 1.275526, 		 Training Loss: 1.265231Epoch 614 		 Validation Loss: 1.275534, 		 Training Loss: 1.261689Validation Loss Decreased(71.428929--->71.428712) Saving model ...
Epoch 615 		 Validation Loss: 1.275513, 		 Training Loss: 1.265151Epoch 616 		 Validation Loss: 1.275519, 		 Training Loss: 1.261163Epoch 617 		 Validation Loss: 1.275514, 		 Training Loss: 1.264896Validation Loss Decreased(71.428712--->71.427528) Saving model ...
Epoch 618 		 Validation Loss: 1.275492, 		 Training Loss: 1.261379Validation Loss Decreased(71.427528--->71.425704) Saving model ...
Epoch 619 		 Validation Loss: 1.275459, 		 Training Loss: 1.277060Validation Loss Decreased(71.425704--->71.422362) Saving model ...
Epoch 620 		 Validation Loss: 1.275399, 		 Training Loss: 1.274614Epoch 621 		 Validation Loss: 1.275428, 		 Training Loss: 1.257849Epoch 622 		 Validation Loss: 1.275402, 		 Training Loss: 1.264557Validation Loss Decreased(71.422362--->71.419500) Saving model ...
Epoch 623 		 Validation Loss: 1.275348, 		 Training Loss: 1.282088Epoch 624 		 Validation Loss: 1.275399, 		 Training Loss: 1.265884Epoch 625 		 Validation Loss: 1.275387, 		 Training Loss: 1.258305Epoch 626 		 Validation Loss: 1.275385, 		 Training Loss: 1.257502Epoch 627 		 Validation Loss: 1.275360, 		 Training Loss: 1.270997Epoch 628 		 Validation Loss: 1.275405, 		 Training Loss: 1.273507Validation Loss Decreased(71.419500--->71.417874) Saving model ...
Epoch 629 		 Validation Loss: 1.275319, 		 Training Loss: 1.264453Validation Loss Decreased(71.417874--->71.417146) Saving model ...
Epoch 630 		 Validation Loss: 1.275306, 		 Training Loss: 1.274239Validation Loss Decreased(71.417146--->71.414874) Saving model ...
Epoch 631 		 Validation Loss: 1.275266, 		 Training Loss: 1.277325Validation Loss Decreased(71.414874--->71.414120) Saving model ...
Epoch 632 		 Validation Loss: 1.275252, 		 Training Loss: 1.266738Epoch 633 		 Validation Loss: 1.275271, 		 Training Loss: 1.260385Epoch 634 		 Validation Loss: 1.275353, 		 Training Loss: 1.256451Epoch 635 		 Validation Loss: 1.275288, 		 Training Loss: 1.265898Epoch 636 		 Validation Loss: 1.275320, 		 Training Loss: 1.254656Epoch 637 		 Validation Loss: 1.275342, 		 Training Loss: 1.267827Epoch 638 		 Validation Loss: 1.275379, 		 Training Loss: 1.268782Epoch 639 		 Validation Loss: 1.275261, 		 Training Loss: 1.257756Epoch 640 		 Validation Loss: 1.275316, 		 Training Loss: 1.254356Epoch 641 		 Validation Loss: 1.275311, 		 Training Loss: 1.264247Epoch 642 		 Validation Loss: 1.275262, 		 Training Loss: 1.270839Epoch 643 		 Validation Loss: 1.275265, 		 Training Loss: 1.257440Epoch 644 		 Validation Loss: 1.275261, 		 Training Loss: 1.261216Epoch 645 		 Validation Loss: 1.275299, 		 Training Loss: 1.261468Epoch 646 		 Validation Loss: 1.275294, 		 Training Loss: 1.258130Epoch 647 		 Validation Loss: 1.275270, 		 Training Loss: 1.269577Epoch 648 		 Validation Loss: 1.275275, 		 Training Loss: 1.272311Epoch 649 		 Validation Loss: 1.275286, 		 Training Loss: 1.258271Epoch 650 		 Validation Loss: 1.275263, 		 Training Loss: 1.265628Validation Loss Decreased(71.414120--->71.413135) Saving model ...
Epoch 651 		 Validation Loss: 1.275235, 		 Training Loss: 1.274646Epoch 652 		 Validation Loss: 1.275255, 		 Training Loss: 1.265690Epoch 653 		 Validation Loss: 1.275270, 		 Training Loss: 1.261522Epoch 654 		 Validation Loss: 1.275255, 		 Training Loss: 1.263701Epoch 655 		 Validation Loss: 1.275334, 		 Training Loss: 1.267836Epoch 656 		 Validation Loss: 1.275384, 		 Training Loss: 1.256741Epoch 657 		 Validation Loss: 1.275278, 		 Training Loss: 1.258841Epoch 658 		 Validation Loss: 1.275328, 		 Training Loss: 1.271040Epoch 659 		 Validation Loss: 1.275247, 		 Training Loss: 1.264405Epoch 660 		 Validation Loss: 1.275247, 		 Training Loss: 1.267859Epoch 661 		 Validation Loss: 1.275256, 		 Training Loss: 1.259051Epoch 662 		 Validation Loss: 1.275241, 		 Training Loss: 1.261876Epoch 663 		 Validation Loss: 1.275248, 		 Training Loss: 1.261347Validation Loss Decreased(71.413135--->71.412479) Saving model ...
Epoch 664 		 Validation Loss: 1.275223, 		 Training Loss: 1.267889Validation Loss Decreased(71.412479--->71.410675) Saving model ...
Epoch 665 		 Validation Loss: 1.275191, 		 Training Loss: 1.270245Validation Loss Decreased(71.410675--->71.410022) Saving model ...
Epoch 666 		 Validation Loss: 1.275179, 		 Training Loss: 1.271074Epoch 667 		 Validation Loss: 1.275204, 		 Training Loss: 1.263037Epoch 668 		 Validation Loss: 1.275205, 		 Training Loss: 1.268358Epoch 669 		 Validation Loss: 1.275203, 		 Training Loss: 1.259828Epoch 670 		 Validation Loss: 1.275199, 		 Training Loss: 1.260367Epoch 671 		 Validation Loss: 1.275196, 		 Training Loss: 1.258535Validation Loss Decreased(71.410022--->71.408904) Saving model ...
Epoch 672 		 Validation Loss: 1.275159, 		 Training Loss: 1.272112Validation Loss Decreased(71.408904--->71.407124) Saving model ...
Epoch 673 		 Validation Loss: 1.275127, 		 Training Loss: 1.261454Epoch 674 		 Validation Loss: 1.275194, 		 Training Loss: 1.258661Epoch 675 		 Validation Loss: 1.275179, 		 Training Loss: 1.263948Epoch 676 		 Validation Loss: 1.275166, 		 Training Loss: 1.270182Epoch 677 		 Validation Loss: 1.275131, 		 Training Loss: 1.265458Validation Loss Decreased(71.407124--->71.406667) Saving model ...
Epoch 678 		 Validation Loss: 1.275119, 		 Training Loss: 1.262585Validation Loss Decreased(71.406667--->71.406481) Saving model ...
Epoch 679 		 Validation Loss: 1.275116, 		 Training Loss: 1.262102Validation Loss Decreased(71.406481--->71.404344) Saving model ...
Epoch 680 		 Validation Loss: 1.275078, 		 Training Loss: 1.275991Epoch 681 		 Validation Loss: 1.275094, 		 Training Loss: 1.265185Epoch 682 		 Validation Loss: 1.275096, 		 Training Loss: 1.264528Epoch 683 		 Validation Loss: 1.275166, 		 Training Loss: 1.255390Epoch 684 		 Validation Loss: 1.275108, 		 Training Loss: 1.255341Epoch 685 		 Validation Loss: 1.275184, 		 Training Loss: 1.268832Epoch 686 		 Validation Loss: 1.275119, 		 Training Loss: 1.260683Epoch 687 		 Validation Loss: 1.275095, 		 Training Loss: 1.270370Epoch 688 		 Validation Loss: 1.275094, 		 Training Loss: 1.270097Validation Loss Decreased(71.404344--->71.404167) Saving model ...
Epoch 689 		 Validation Loss: 1.275074, 		 Training Loss: 1.259342Epoch 690 		 Validation Loss: 1.275088, 		 Training Loss: 1.263865Epoch 691 		 Validation Loss: 1.275122, 		 Training Loss: 1.257001Epoch 692 		 Validation Loss: 1.275137, 		 Training Loss: 1.255704Epoch 693 		 Validation Loss: 1.275150, 		 Training Loss: 1.256873Epoch 694 		 Validation Loss: 1.275101, 		 Training Loss: 1.275370Epoch 695 		 Validation Loss: 1.275144, 		 Training Loss: 1.259135Epoch 696 		 Validation Loss: 1.275094, 		 Training Loss: 1.270524Validation Loss Decreased(71.404167--->71.402683) Saving model ...
Epoch 697 		 Validation Loss: 1.275048, 		 Training Loss: 1.269297Epoch 698 		 Validation Loss: 1.275064, 		 Training Loss: 1.259598Epoch 699 		 Validation Loss: 1.275144, 		 Training Loss: 1.255097Epoch 700 		 Validation Loss: 1.275096, 		 Training Loss: 1.259159Epoch 701 		 Validation Loss: 1.275120, 		 Training Loss: 1.263561Epoch 702 		 Validation Loss: 1.275103, 		 Training Loss: 1.259368Epoch 703 		 Validation Loss: 1.275135, 		 Training Loss: 1.259312Epoch 704 		 Validation Loss: 1.275115, 		 Training Loss: 1.267973Epoch 705 		 Validation Loss: 1.275085, 		 Training Loss: 1.268546Epoch 706 		 Validation Loss: 1.275126, 		 Training Loss: 1.257797Epoch 707 		 Validation Loss: 1.275147, 		 Training Loss: 1.263882Epoch 708 		 Validation Loss: 1.275119, 		 Training Loss: 1.266768Epoch 709 		 Validation Loss: 1.275148, 		 Training Loss: 1.251495Epoch 710 		 Validation Loss: 1.275131, 		 Training Loss: 1.265317Epoch 711 		 Validation Loss: 1.275076, 		 Training Loss: 1.267998Epoch 712 		 Validation Loss: 1.275101, 		 Training Loss: 1.263988Epoch 713 		 Validation Loss: 1.275071, 		 Training Loss: 1.267692Epoch 714 		 Validation Loss: 1.275081, 		 Training Loss: 1.270035Epoch 715 		 Validation Loss: 1.275050, 		 Training Loss: 1.254689Epoch 716 		 Validation Loss: 1.275072, 		 Training Loss: 1.267262Epoch 717 		 Validation Loss: 1.275096, 		 Training Loss: 1.255789Epoch 718 		 Validation Loss: 1.275086, 		 Training Loss: 1.259469Epoch 719 		 Validation Loss: 1.275115, 		 Training Loss: 1.257310Epoch 720 		 Validation Loss: 1.275172, 		 Training Loss: 1.259526Epoch 721 		 Validation Loss: 1.275136, 		 Training Loss: 1.258050Epoch 722 		 Validation Loss: 1.275143, 		 Training Loss: 1.264950Epoch 723 		 Validation Loss: 1.275154, 		 Training Loss: 1.259839Epoch 724 		 Validation Loss: 1.275118, 		 Training Loss: 1.262414Epoch 725 		 Validation Loss: 1.275147, 		 Training Loss: 1.260270Epoch 726 		 Validation Loss: 1.275144, 		 Training Loss: 1.258269Epoch 727 		 Validation Loss: 1.275123, 		 Training Loss: 1.262892Epoch 728 		 Validation Loss: 1.275145, 		 Training Loss: 1.253854Epoch 729 		 Validation Loss: 1.275171, 		 Training Loss: 1.261044Epoch 730 		 Validation Loss: 1.275124, 		 Training Loss: 1.261569Epoch 731 		 Validation Loss: 1.275073, 		 Training Loss: 1.273622Epoch 732 		 Validation Loss: 1.275065, 		 Training Loss: 1.264079Epoch 733 		 Validation Loss: 1.275060, 		 Training Loss: 1.268863Epoch 734 		 Validation Loss: 1.275072, 		 Training Loss: 1.262866Epoch 735 		 Validation Loss: 1.275065, 		 Training Loss: 1.261389Epoch 736 		 Validation Loss: 1.275053, 		 Training Loss: 1.268743Validation Loss Decreased(71.402683--->71.402045) Saving model ...
Epoch 737 		 Validation Loss: 1.275037, 		 Training Loss: 1.263296Validation Loss Decreased(71.402045--->71.401343) Saving model ...
Epoch 738 		 Validation Loss: 1.275024, 		 Training Loss: 1.263669Validation Loss Decreased(71.401343--->71.400680) Saving model ...
Epoch 739 		 Validation Loss: 1.275012, 		 Training Loss: 1.258758Epoch 740 		 Validation Loss: 1.275028, 		 Training Loss: 1.262835Validation Loss Decreased(71.400680--->71.398507) Saving model ...
Epoch 741 		 Validation Loss: 1.274973, 		 Training Loss: 1.279023Epoch 742 		 Validation Loss: 1.274980, 		 Training Loss: 1.252543Epoch 743 		 Validation Loss: 1.275011, 		 Training Loss: 1.260089Epoch 744 		 Validation Loss: 1.275028, 		 Training Loss: 1.267055Epoch 745 		 Validation Loss: 1.275045, 		 Training Loss: 1.264786Epoch 746 		 Validation Loss: 1.275016, 		 Training Loss: 1.260588Epoch 747 		 Validation Loss: 1.275031, 		 Training Loss: 1.260052Epoch 748 		 Validation Loss: 1.275010, 		 Training Loss: 1.277206Epoch 749 		 Validation Loss: 1.275001, 		 Training Loss: 1.267930Validation Loss Decreased(71.398507--->71.397781) Saving model ...
Epoch 750 		 Validation Loss: 1.274960, 		 Training Loss: 1.259548Epoch 751 		 Validation Loss: 1.274996, 		 Training Loss: 1.267367Validation Loss Decreased(71.397781--->71.397457) Saving model ...
Epoch 752 		 Validation Loss: 1.274955, 		 Training Loss: 1.260040Epoch 753 		 Validation Loss: 1.274998, 		 Training Loss: 1.259151Epoch 754 		 Validation Loss: 1.274966, 		 Training Loss: 1.269353Validation Loss Decreased(71.397457--->71.397111) Saving model ...
Epoch 755 		 Validation Loss: 1.274948, 		 Training Loss: 1.270491Epoch 756 		 Validation Loss: 1.274970, 		 Training Loss: 1.257589Validation Loss Decreased(71.397111--->71.396438) Saving model ...
Epoch 757 		 Validation Loss: 1.274936, 		 Training Loss: 1.274226Validation Loss Decreased(71.396438--->71.393428) Saving model ...
Epoch 758 		 Validation Loss: 1.274883, 		 Training Loss: 1.258311Epoch 759 		 Validation Loss: 1.274921, 		 Training Loss: 1.251454Epoch 760 		 Validation Loss: 1.274901, 		 Training Loss: 1.260682Epoch 761 		 Validation Loss: 1.274970, 		 Training Loss: 1.259279Epoch 762 		 Validation Loss: 1.274985, 		 Training Loss: 1.266757Epoch 763 		 Validation Loss: 1.274954, 		 Training Loss: 1.259956Epoch 764 		 Validation Loss: 1.274937, 		 Training Loss: 1.267759Validation Loss Decreased(71.393428--->71.392809) Saving model ...
Epoch 765 		 Validation Loss: 1.274872, 		 Training Loss: 1.257997Epoch 766 		 Validation Loss: 1.274934, 		 Training Loss: 1.258067Epoch 767 		 Validation Loss: 1.274912, 		 Training Loss: 1.255051Validation Loss Decreased(71.392809--->71.392341) Saving model ...
Epoch 768 		 Validation Loss: 1.274863, 		 Training Loss: 1.279691Validation Loss Decreased(71.392341--->71.392008) Saving model ...
Epoch 769 		 Validation Loss: 1.274857, 		 Training Loss: 1.258286Validation Loss Decreased(71.392008--->71.391798) Saving model ...
Epoch 770 		 Validation Loss: 1.274854, 		 Training Loss: 1.263058Epoch 771 		 Validation Loss: 1.274880, 		 Training Loss: 1.257691Epoch 772 		 Validation Loss: 1.274914, 		 Training Loss: 1.262502Epoch 773 		 Validation Loss: 1.274886, 		 Training Loss: 1.263334Validation Loss Decreased(71.391798--->71.391469) Saving model ...
Epoch 774 		 Validation Loss: 1.274848, 		 Training Loss: 1.266277Epoch 775 		 Validation Loss: 1.274854, 		 Training Loss: 1.261304Validation Loss Decreased(71.391469--->71.389904) Saving model ...
Epoch 776 		 Validation Loss: 1.274820, 		 Training Loss: 1.272086Epoch 777 		 Validation Loss: 1.274833, 		 Training Loss: 1.264167Epoch 778 		 Validation Loss: 1.274933, 		 Training Loss: 1.259263Epoch 779 		 Validation Loss: 1.274903, 		 Training Loss: 1.260179Epoch 780 		 Validation Loss: 1.274890, 		 Training Loss: 1.260402Epoch 781 		 Validation Loss: 1.274903, 		 Training Loss: 1.272662Epoch 782 		 Validation Loss: 1.274905, 		 Training Loss: 1.260467Epoch 783 		 Validation Loss: 1.274893, 		 Training Loss: 1.258904Epoch 784 		 Validation Loss: 1.274860, 		 Training Loss: 1.266923Validation Loss Decreased(71.389904--->71.389063) Saving model ...
Epoch 785 		 Validation Loss: 1.274805, 		 Training Loss: 1.266538Epoch 786 		 Validation Loss: 1.274813, 		 Training Loss: 1.268048Epoch 787 		 Validation Loss: 1.274824, 		 Training Loss: 1.261159Epoch 788 		 Validation Loss: 1.274865, 		 Training Loss: 1.258466Epoch 789 		 Validation Loss: 1.274825, 		 Training Loss: 1.271565Epoch 790 		 Validation Loss: 1.274813, 		 Training Loss: 1.264561Epoch 791 		 Validation Loss: 1.274830, 		 Training Loss: 1.259594Epoch 792 		 Validation Loss: 1.274848, 		 Training Loss: 1.258993Epoch 793 		 Validation Loss: 1.274864, 		 Training Loss: 1.256660Epoch 794 		 Validation Loss: 1.274845, 		 Training Loss: 1.271106Epoch 795 		 Validation Loss: 1.274843, 		 Training Loss: 1.262182Epoch 796 		 Validation Loss: 1.274882, 		 Training Loss: 1.263251Epoch 797 		 Validation Loss: 1.274847, 		 Training Loss: 1.269637Epoch 798 		 Validation Loss: 1.274820, 		 Training Loss: 1.257064Validation Loss Decreased(71.389063--->71.388631) Saving model ...
Epoch 799 		 Validation Loss: 1.274797, 		 Training Loss: 1.256772Epoch 800 		 Validation Loss: 1.274799, 		 Training Loss: 1.264653Validation Loss Decreased(71.388631--->71.387865) Saving model ...
Epoch 801 		 Validation Loss: 1.274783, 		 Training Loss: 1.265007Epoch 802 		 Validation Loss: 1.274807, 		 Training Loss: 1.257924Validation Loss Decreased(71.387865--->71.387677) Saving model ...
Epoch 803 		 Validation Loss: 1.274780, 		 Training Loss: 1.270707Validation Loss Decreased(71.387677--->71.385660) Saving model ...
Epoch 804 		 Validation Loss: 1.274744, 		 Training Loss: 1.270675Validation Loss Decreased(71.385660--->71.385289) Saving model ...
Epoch 805 		 Validation Loss: 1.274737, 		 Training Loss: 1.263754Validation Loss Decreased(71.385289--->71.384717) Saving model ...
Epoch 806 		 Validation Loss: 1.274727, 		 Training Loss: 1.258394Epoch 807 		 Validation Loss: 1.274760, 		 Training Loss: 1.268394Epoch 808 		 Validation Loss: 1.274735, 		 Training Loss: 1.264557Epoch 809 		 Validation Loss: 1.274742, 		 Training Loss: 1.262195Epoch 810 		 Validation Loss: 1.274758, 		 Training Loss: 1.252712Epoch 811 		 Validation Loss: 1.274830, 		 Training Loss: 1.270169Epoch 812 		 Validation Loss: 1.274755, 		 Training Loss: 1.265273Epoch 813 		 Validation Loss: 1.274816, 		 Training Loss: 1.259080Epoch 814 		 Validation Loss: 1.274802, 		 Training Loss: 1.257876Epoch 815 		 Validation Loss: 1.274798, 		 Training Loss: 1.257678Epoch 816 		 Validation Loss: 1.274819, 		 Training Loss: 1.251387Epoch 817 		 Validation Loss: 1.274830, 		 Training Loss: 1.254238Epoch 818 		 Validation Loss: 1.274820, 		 Training Loss: 1.264602Epoch 819 		 Validation Loss: 1.274832, 		 Training Loss: 1.260710Epoch 820 		 Validation Loss: 1.274828, 		 Training Loss: 1.256664Epoch 821 		 Validation Loss: 1.274815, 		 Training Loss: 1.261659Epoch 822 		 Validation Loss: 1.274934, 		 Training Loss: 1.268107Epoch 823 		 Validation Loss: 1.274764, 		 Training Loss: 1.266605Epoch 824 		 Validation Loss: 1.274786, 		 Training Loss: 1.276628Epoch 825 		 Validation Loss: 1.274731, 		 Training Loss: 1.259717Validation Loss Decreased(71.384717--->71.384498) Saving model ...
Epoch 826 		 Validation Loss: 1.274723, 		 Training Loss: 1.267897Epoch 827 		 Validation Loss: 1.274760, 		 Training Loss: 1.258240Epoch 828 		 Validation Loss: 1.274729, 		 Training Loss: 1.256832Epoch 829 		 Validation Loss: 1.274791, 		 Training Loss: 1.255778Epoch 830 		 Validation Loss: 1.274729, 		 Training Loss: 1.261094Epoch 831 		 Validation Loss: 1.274751, 		 Training Loss: 1.261672Validation Loss Decreased(71.384498--->71.382662) Saving model ...
Epoch 832 		 Validation Loss: 1.274690, 		 Training Loss: 1.264797Validation Loss Decreased(71.382662--->71.378425) Saving model ...
Epoch 833 		 Validation Loss: 1.274615, 		 Training Loss: 1.290890Validation Loss Decreased(71.378425--->71.377061) Saving model ...
Epoch 834 		 Validation Loss: 1.274590, 		 Training Loss: 1.267304Validation Loss Decreased(71.377061--->71.376790) Saving model ...
Epoch 835 		 Validation Loss: 1.274586, 		 Training Loss: 1.262780Epoch 836 		 Validation Loss: 1.274589, 		 Training Loss: 1.259462Validation Loss Decreased(71.376790--->71.376089) Saving model ...
Epoch 837 		 Validation Loss: 1.274573, 		 Training Loss: 1.270519Epoch 838 		 Validation Loss: 1.274597, 		 Training Loss: 1.268222Epoch 839 		 Validation Loss: 1.274605, 		 Training Loss: 1.264320Epoch 840 		 Validation Loss: 1.274579, 		 Training Loss: 1.257986Epoch 841 		 Validation Loss: 1.274575, 		 Training Loss: 1.277382Validation Loss Decreased(71.376089--->71.373760) Saving model ...
Epoch 842 		 Validation Loss: 1.274531, 		 Training Loss: 1.269725Validation Loss Decreased(71.373760--->71.372284) Saving model ...
Epoch 843 		 Validation Loss: 1.274505, 		 Training Loss: 1.262940Epoch 844 		 Validation Loss: 1.274522, 		 Training Loss: 1.260791Epoch 845 		 Validation Loss: 1.274517, 		 Training Loss: 1.264714Epoch 846 		 Validation Loss: 1.274508, 		 Training Loss: 1.260781Validation Loss Decreased(71.372284--->71.371803) Saving model ...
Epoch 847 		 Validation Loss: 1.274496, 		 Training Loss: 1.267842Epoch 848 		 Validation Loss: 1.274513, 		 Training Loss: 1.257673Epoch 849 		 Validation Loss: 1.274639, 		 Training Loss: 1.258621Epoch 850 		 Validation Loss: 1.274584, 		 Training Loss: 1.259936Epoch 851 		 Validation Loss: 1.274569, 		 Training Loss: 1.279239Validation Loss Decreased(71.371803--->71.370916) Saving model ...
Epoch 852 		 Validation Loss: 1.274481, 		 Training Loss: 1.268903Epoch 853 		 Validation Loss: 1.274488, 		 Training Loss: 1.257740Epoch 854 		 Validation Loss: 1.274507, 		 Training Loss: 1.259905Epoch 855 		 Validation Loss: 1.274526, 		 Training Loss: 1.269497Epoch 856 		 Validation Loss: 1.274495, 		 Training Loss: 1.254884Validation Loss Decreased(71.370916--->71.370326) Saving model ...
Epoch 857 		 Validation Loss: 1.274470, 		 Training Loss: 1.272080Epoch 858 		 Validation Loss: 1.274487, 		 Training Loss: 1.270269Validation Loss Decreased(71.370326--->71.364646) Saving model ...
Epoch 859 		 Validation Loss: 1.274369, 		 Training Loss: 1.269122Validation Loss Decreased(71.364646--->71.363338) Saving model ...
Epoch 860 		 Validation Loss: 1.274345, 		 Training Loss: 1.263738Validation Loss Decreased(71.363338--->71.363302) Saving model ...
Epoch 861 		 Validation Loss: 1.274345, 		 Training Loss: 1.272360Epoch 862 		 Validation Loss: 1.274374, 		 Training Loss: 1.267680Epoch 863 		 Validation Loss: 1.274372, 		 Training Loss: 1.258003Epoch 864 		 Validation Loss: 1.274378, 		 Training Loss: 1.255663Epoch 865 		 Validation Loss: 1.274388, 		 Training Loss: 1.261151Epoch 866 		 Validation Loss: 1.274418, 		 Training Loss: 1.260919Epoch 867 		 Validation Loss: 1.274433, 		 Training Loss: 1.258039Epoch 868 		 Validation Loss: 1.274432, 		 Training Loss: 1.262925Epoch 869 		 Validation Loss: 1.274455, 		 Training Loss: 1.262473Epoch 870 		 Validation Loss: 1.274434, 		 Training Loss: 1.262278Epoch 871 		 Validation Loss: 1.274476, 		 Training Loss: 1.257670Epoch 872 		 Validation Loss: 1.274440, 		 Training Loss: 1.268242Epoch 873 		 Validation Loss: 1.274395, 		 Training Loss: 1.282143Epoch 874 		 Validation Loss: 1.274400, 		 Training Loss: 1.254669Epoch 875 		 Validation Loss: 1.274400, 		 Training Loss: 1.267574Epoch 876 		 Validation Loss: 1.274383, 		 Training Loss: 1.271133Epoch 877 		 Validation Loss: 1.274374, 		 Training Loss: 1.257944Epoch 878 		 Validation Loss: 1.274419, 		 Training Loss: 1.262768Epoch 879 		 Validation Loss: 1.274407, 		 Training Loss: 1.254777Epoch 880 		 Validation Loss: 1.274413, 		 Training Loss: 1.256569Epoch 881 		 Validation Loss: 1.274399, 		 Training Loss: 1.267222Epoch 882 		 Validation Loss: 1.274368, 		 Training Loss: 1.261749Epoch 883 		 Validation Loss: 1.274431, 		 Training Loss: 1.264526Epoch 884 		 Validation Loss: 1.274366, 		 Training Loss: 1.263925Epoch 885 		 Validation Loss: 1.274399, 		 Training Loss: 1.259490Epoch 886 		 Validation Loss: 1.274398, 		 Training Loss: 1.275883Epoch 887 		 Validation Loss: 1.274387, 		 Training Loss: 1.261423Validation Loss Decreased(71.363302--->71.361018) Saving model ...
Epoch 888 		 Validation Loss: 1.274304, 		 Training Loss: 1.262187Epoch 889 		 Validation Loss: 1.274323, 		 Training Loss: 1.253995Epoch 890 		 Validation Loss: 1.274358, 		 Training Loss: 1.255589Epoch 891 		 Validation Loss: 1.274322, 		 Training Loss: 1.277501Epoch 892 		 Validation Loss: 1.274413, 		 Training Loss: 1.254296Validation Loss Decreased(71.361018--->71.360439) Saving model ...
Epoch 893 		 Validation Loss: 1.274294, 		 Training Loss: 1.262871Validation Loss Decreased(71.360439--->71.359999) Saving model ...
Epoch 894 		 Validation Loss: 1.274286, 		 Training Loss: 1.271350Epoch 895 		 Validation Loss: 1.274300, 		 Training Loss: 1.276221Validation Loss Decreased(71.359999--->71.358586) Saving model ...
Epoch 896 		 Validation Loss: 1.274260, 		 Training Loss: 1.262490Epoch 897 		 Validation Loss: 1.274322, 		 Training Loss: 1.260062Validation Loss Decreased(71.358586--->71.358567) Saving model ...
Epoch 898 		 Validation Loss: 1.274260, 		 Training Loss: 1.259433Validation Loss Decreased(71.358567--->71.357810) Saving model ...
Epoch 899 		 Validation Loss: 1.274247, 		 Training Loss: 1.260385Validation Loss Decreased(71.357810--->71.357272) Saving model ...
Epoch 900 		 Validation Loss: 1.274237, 		 Training Loss: 1.263700Epoch 901 		 Validation Loss: 1.274250, 		 Training Loss: 1.264403Validation Loss Decreased(71.357272--->71.355777) Saving model ...
Epoch 902 		 Validation Loss: 1.274210, 		 Training Loss: 1.263548Epoch 903 		 Validation Loss: 1.274212, 		 Training Loss: 1.257038Epoch 904 		 Validation Loss: 1.274212, 		 Training Loss: 1.259955Epoch 905 		 Validation Loss: 1.274360, 		 Training Loss: 1.258464Validation Loss Decreased(71.355777--->71.355276) Saving model ...
Epoch 906 		 Validation Loss: 1.274201, 		 Training Loss: 1.265030Validation Loss Decreased(71.355276--->71.353292) Saving model ...
Epoch 907 		 Validation Loss: 1.274166, 		 Training Loss: 1.273208Epoch 908 		 Validation Loss: 1.274215, 		 Training Loss: 1.254953Epoch 909 		 Validation Loss: 1.274181, 		 Training Loss: 1.263264Epoch 910 		 Validation Loss: 1.274212, 		 Training Loss: 1.259931Epoch 911 		 Validation Loss: 1.274171, 		 Training Loss: 1.265837Epoch 912 		 Validation Loss: 1.274190, 		 Training Loss: 1.263755Epoch 913 		 Validation Loss: 1.274172, 		 Training Loss: 1.264230Epoch 914 		 Validation Loss: 1.274199, 		 Training Loss: 1.263612Epoch 915 		 Validation Loss: 1.274215, 		 Training Loss: 1.257245Epoch 916 		 Validation Loss: 1.274264, 		 Training Loss: 1.263754Epoch 917 		 Validation Loss: 1.274243, 		 Training Loss: 1.257952Epoch 918 		 Validation Loss: 1.274230, 		 Training Loss: 1.256859Epoch 919 		 Validation Loss: 1.274185, 		 Training Loss: 1.273960Epoch 920 		 Validation Loss: 1.274210, 		 Training Loss: 1.260516Epoch 921 		 Validation Loss: 1.274197, 		 Training Loss: 1.260399Epoch 922 		 Validation Loss: 1.274204, 		 Training Loss: 1.249525Epoch 923 		 Validation Loss: 1.274209, 		 Training Loss: 1.251780Epoch 924 		 Validation Loss: 1.274205, 		 Training Loss: 1.264164Epoch 925 		 Validation Loss: 1.274187, 		 Training Loss: 1.260989Epoch 926 		 Validation Loss: 1.274254, 		 Training Loss: 1.276007Validation Loss Decreased(71.353292--->71.353131) Saving model ...
Epoch 927 		 Validation Loss: 1.274163, 		 Training Loss: 1.262870Epoch 928 		 Validation Loss: 1.274213, 		 Training Loss: 1.254060Epoch 929 		 Validation Loss: 1.274259, 		 Training Loss: 1.270656Epoch 930 		 Validation Loss: 1.274209, 		 Training Loss: 1.258239Validation Loss Decreased(71.353131--->71.352125) Saving model ...
Epoch 931 		 Validation Loss: 1.274145, 		 Training Loss: 1.265856Epoch 932 		 Validation Loss: 1.274164, 		 Training Loss: 1.263283Epoch 933 		 Validation Loss: 1.274148, 		 Training Loss: 1.258318Validation Loss Decreased(71.352125--->71.351896) Saving model ...
Epoch 934 		 Validation Loss: 1.274141, 		 Training Loss: 1.253934Epoch 935 		 Validation Loss: 1.274185, 		 Training Loss: 1.254785Epoch 936 		 Validation Loss: 1.274155, 		 Training Loss: 1.261826Epoch 937 		 Validation Loss: 1.274163, 		 Training Loss: 1.263804Epoch 938 		 Validation Loss: 1.274152, 		 Training Loss: 1.271193Validation Loss Decreased(71.351896--->71.351772) Saving model ...
Epoch 939 		 Validation Loss: 1.274139, 		 Training Loss: 1.262113Epoch 940 		 Validation Loss: 1.274140, 		 Training Loss: 1.264619Validation Loss Decreased(71.351772--->71.350994) Saving model ...
Epoch 941 		 Validation Loss: 1.274125, 		 Training Loss: 1.267937Validation Loss Decreased(71.350994--->71.349789) Saving model ...
Epoch 942 		 Validation Loss: 1.274103, 		 Training Loss: 1.264033Validation Loss Decreased(71.349789--->71.349124) Saving model ...
Epoch 943 		 Validation Loss: 1.274092, 		 Training Loss: 1.265896Validation Loss Decreased(71.349124--->71.348279) Saving model ...
Epoch 944 		 Validation Loss: 1.274076, 		 Training Loss: 1.259772Epoch 945 		 Validation Loss: 1.274163, 		 Training Loss: 1.267540Validation Loss Decreased(71.348279--->71.347613) Saving model ...
Epoch 946 		 Validation Loss: 1.274065, 		 Training Loss: 1.257719Epoch 947 		 Validation Loss: 1.274077, 		 Training Loss: 1.263892Validation Loss Decreased(71.347613--->71.345904) Saving model ...
Epoch 948 		 Validation Loss: 1.274034, 		 Training Loss: 1.265801Validation Loss Decreased(71.345904--->71.345713) Saving model ...
Epoch 949 		 Validation Loss: 1.274031, 		 Training Loss: 1.264655Epoch 950 		 Validation Loss: 1.274043, 		 Training Loss: 1.257172Epoch 951 		 Validation Loss: 1.274087, 		 Training Loss: 1.257668Epoch 952 		 Validation Loss: 1.274107, 		 Training Loss: 1.258657Epoch 953 		 Validation Loss: 1.274133, 		 Training Loss: 1.260604Epoch 954 		 Validation Loss: 1.274072, 		 Training Loss: 1.264878Epoch 955 		 Validation Loss: 1.274102, 		 Training Loss: 1.266328Epoch 956 		 Validation Loss: 1.274109, 		 Training Loss: 1.254598Epoch 957 		 Validation Loss: 1.274108, 		 Training Loss: 1.260352Epoch 958 		 Validation Loss: 1.274110, 		 Training Loss: 1.263284Epoch 959 		 Validation Loss: 1.274157, 		 Training Loss: 1.269218Epoch 960 		 Validation Loss: 1.274117, 		 Training Loss: 1.260017Epoch 961 		 Validation Loss: 1.274072, 		 Training Loss: 1.269263Epoch 962 		 Validation Loss: 1.274066, 		 Training Loss: 1.265610Epoch 963 		 Validation Loss: 1.274137, 		 Training Loss: 1.263356Epoch 964 		 Validation Loss: 1.274052, 		 Training Loss: 1.265680Epoch 965 		 Validation Loss: 1.274049, 		 Training Loss: 1.273734Epoch 966 		 Validation Loss: 1.274035, 		 Training Loss: 1.268831Epoch 967 		 Validation Loss: 1.274089, 		 Training Loss: 1.255561Epoch 968 		 Validation Loss: 1.274088, 		 Training Loss: 1.256700Validation Loss Decreased(71.345713--->71.343681) Saving model ...
Epoch 969 		 Validation Loss: 1.273994, 		 Training Loss: 1.262723Epoch 970 		 Validation Loss: 1.274000, 		 Training Loss: 1.265136Epoch 971 		 Validation Loss: 1.274005, 		 Training Loss: 1.250785Epoch 972 		 Validation Loss: 1.273997, 		 Training Loss: 1.262238Validation Loss Decreased(71.343681--->71.342949) Saving model ...
Epoch 973 		 Validation Loss: 1.273981, 		 Training Loss: 1.264667Epoch 974 		 Validation Loss: 1.273992, 		 Training Loss: 1.263014Epoch 975 		 Validation Loss: 1.274024, 		 Training Loss: 1.264922Epoch 976 		 Validation Loss: 1.273985, 		 Training Loss: 1.260490Epoch 977 		 Validation Loss: 1.274036, 		 Training Loss: 1.261790Validation Loss Decreased(71.342949--->71.342432) Saving model ...
Epoch 978 		 Validation Loss: 1.273972, 		 Training Loss: 1.272578Validation Loss Decreased(71.342432--->71.341094) Saving model ...
Epoch 979 		 Validation Loss: 1.273948, 		 Training Loss: 1.255490Validation Loss Decreased(71.341094--->71.340319) Saving model ...
Epoch 980 		 Validation Loss: 1.273934, 		 Training Loss: 1.267995Epoch 981 		 Validation Loss: 1.273937, 		 Training Loss: 1.266653Validation Loss Decreased(71.340319--->71.337437) Saving model ...
Epoch 982 		 Validation Loss: 1.273883, 		 Training Loss: 1.274904Epoch 983 		 Validation Loss: 1.273984, 		 Training Loss: 1.259978Epoch 984 		 Validation Loss: 1.273920, 		 Training Loss: 1.263517Epoch 985 		 Validation Loss: 1.273899, 		 Training Loss: 1.265121Validation Loss Decreased(71.337437--->71.336566) Saving model ...
Epoch 986 		 Validation Loss: 1.273867, 		 Training Loss: 1.259329Validation Loss Decreased(71.336566--->71.336506) Saving model ...
Epoch 987 		 Validation Loss: 1.273866, 		 Training Loss: 1.260485Epoch 988 		 Validation Loss: 1.273897, 		 Training Loss: 1.265906Validation Loss Decreased(71.336506--->71.334818) Saving model ...
Epoch 989 		 Validation Loss: 1.273836, 		 Training Loss: 1.263480Epoch 990 		 Validation Loss: 1.273846, 		 Training Loss: 1.254815Epoch 991 		 Validation Loss: 1.273869, 		 Training Loss: 1.259242Epoch 992 		 Validation Loss: 1.273839, 		 Training Loss: 1.263068Epoch 993 		 Validation Loss: 1.273903, 		 Training Loss: 1.260597Epoch 994 		 Validation Loss: 1.273892, 		 Training Loss: 1.260418Epoch 995 		 Validation Loss: 1.273845, 		 Training Loss: 1.268677Epoch 996 		 Validation Loss: 1.273875, 		 Training Loss: 1.255939Epoch 997 		 Validation Loss: 1.273839, 		 Training Loss: 1.266467Validation Loss Decreased(71.334818--->71.333930) Saving model ...
Epoch 998 		 Validation Loss: 1.273820, 		 Training Loss: 1.261634Epoch 999 		 Validation Loss: 1.273833, 		 Training Loss: 1.265811Validation Loss Decreased(71.333930--->71.332656) Saving model ...
Epoch 1000 		 Validation Loss: 1.273797, 		 Training Loss: 1.269522                         Training Completed!
#-----------------------------------------------------------------------#
