#-----------------------------------------------------------------------#
                     Initializing training process
-------------------------------------------------------------------------

Torch version: 1.11.0 ------ Selected Device: cuda:0
Sample Rate: 16000 Hz ------  Crop Lenght: 3200 samples
-------------------------------------------------------------------------
Found 4 files in data/plate-spring/spring
Using dry_train.h5 and wet_train.h5 for train split.
-------------------------------------------------------------------------
Parameters: 247.688 k
Receptive field: 54611 samples or 3413.2 ms
-------------------------------------------------------------------------
Validation Loss Decreased(inf--->102.786917) Saving model ...
Epoch 1 		 Validation Loss: 1.835481, 		 Training Loss: 82.770360Validation Loss Decreased(102.786917--->80.521990) Saving model ...
Epoch 2 		 Validation Loss: 1.437893, 		 Training Loss: 1.528574Validation Loss Decreased(80.521990--->77.919130) Saving model ...
Epoch 3 		 Validation Loss: 1.391413, 		 Training Loss: 1.386184Validation Loss Decreased(77.919130--->74.165570) Saving model ...
Epoch 4 		 Validation Loss: 1.324385, 		 Training Loss: 1.362872Epoch 5 		 Validation Loss: 1.371535, 		 Training Loss: 1.342740Epoch 6 		 Validation Loss: 1.383567, 		 Training Loss: 1.326473Epoch 7 		 Validation Loss: 1.694717, 		 Training Loss: 1.581019Validation Loss Decreased(74.165570--->73.955759) Saving model ...
Epoch 8 		 Validation Loss: 1.320639, 		 Training Loss: 1.327857Epoch 9 		 Validation Loss: 1.364721, 		 Training Loss: 1.315907Validation Loss Decreased(73.955759--->72.798169) Saving model ...
Epoch 10 		 Validation Loss: 1.299967, 		 Training Loss: 1.321465Validation Loss Decreased(72.798169--->72.590076) Saving model ...
Epoch 11 		 Validation Loss: 1.296251, 		 Training Loss: 1.291384Epoch 12 		 Validation Loss: 1.304283, 		 Training Loss: 1.277140Validation Loss Decreased(72.590076--->72.468170) Saving model ...
Epoch 13 		 Validation Loss: 1.294074, 		 Training Loss: 1.271165Validation Loss Decreased(72.468170--->72.255831) Saving model ...
Epoch 14 		 Validation Loss: 1.290283, 		 Training Loss: 1.274668Epoch 15 		 Validation Loss: 1.297643, 		 Training Loss: 1.277885Epoch 16 		 Validation Loss: 1.295946, 		 Training Loss: 1.269665Epoch 17 		 Validation Loss: 1.290307, 		 Training Loss: 1.284356Validation Loss Decreased(72.255831--->72.129434) Saving model ...
Epoch 18 		 Validation Loss: 1.288026, 		 Training Loss: 1.259181Validation Loss Decreased(72.129434--->71.735014) Saving model ...
Epoch 19 		 Validation Loss: 1.280982, 		 Training Loss: 1.267291Epoch 20 		 Validation Loss: 1.293408, 		 Training Loss: 1.257347Validation Loss Decreased(71.735014--->71.445309) Saving model ...
Epoch 21 		 Validation Loss: 1.275809, 		 Training Loss: 1.269065Epoch 22 		 Validation Loss: 1.277129, 		 Training Loss: 1.247781Validation Loss Decreased(71.445309--->71.311454) Saving model ...
Epoch 23 		 Validation Loss: 1.273419, 		 Training Loss: 1.253757Validation Loss Decreased(71.311454--->71.289417) Saving model ...
Epoch 24 		 Validation Loss: 1.273025, 		 Training Loss: 1.254757Validation Loss Decreased(71.289417--->71.208336) Saving model ...
Epoch 25 		 Validation Loss: 1.271577, 		 Training Loss: 1.249250Epoch 26 		 Validation Loss: 1.272097, 		 Training Loss: 1.247230Validation Loss Decreased(71.208336--->71.060705) Saving model ...
Epoch 27 		 Validation Loss: 1.268941, 		 Training Loss: 1.256028Validation Loss Decreased(71.060705--->71.041471) Saving model ...
Epoch 28 		 Validation Loss: 1.268598, 		 Training Loss: 1.243463Epoch 29 		 Validation Loss: 1.269837, 		 Training Loss: 1.242195Epoch 30 		 Validation Loss: 1.270941, 		 Training Loss: 1.243357Validation Loss Decreased(71.041471--->70.962090) Saving model ...
Epoch 31 		 Validation Loss: 1.267180, 		 Training Loss: 1.248901Validation Loss Decreased(70.962090--->70.959000) Saving model ...
Epoch 32 		 Validation Loss: 1.267125, 		 Training Loss: 1.253511Validation Loss Decreased(70.959000--->70.936381) Saving model ...
Epoch 33 		 Validation Loss: 1.266721, 		 Training Loss: 1.263084Validation Loss Decreased(70.936381--->70.918839) Saving model ...
Epoch 34 		 Validation Loss: 1.266408, 		 Training Loss: 1.246628Epoch 35 		 Validation Loss: 1.266700, 		 Training Loss: 1.250901Epoch 36 		 Validation Loss: 1.267392, 		 Training Loss: 1.247150Epoch 37 		 Validation Loss: 1.266958, 		 Training Loss: 1.235854Epoch 38 		 Validation Loss: 1.267151, 		 Training Loss: 1.239891Epoch 39 		 Validation Loss: 1.266415, 		 Training Loss: 1.244168Validation Loss Decreased(70.918839--->70.877210) Saving model ...
Epoch 40 		 Validation Loss: 1.265664, 		 Training Loss: 1.256189Validation Loss Decreased(70.877210--->70.874386) Saving model ...
Epoch 41 		 Validation Loss: 1.265614, 		 Training Loss: 1.240994Validation Loss Decreased(70.874386--->70.874097) Saving model ...
Epoch 42 		 Validation Loss: 1.265609, 		 Training Loss: 1.241609Validation Loss Decreased(70.874097--->70.869323) Saving model ...
Epoch 43 		 Validation Loss: 1.265524, 		 Training Loss: 1.246178Epoch 44 		 Validation Loss: 1.265779, 		 Training Loss: 1.249281Epoch 45 		 Validation Loss: 1.265538, 		 Training Loss: 1.236073Epoch 46 		 Validation Loss: 1.265763, 		 Training Loss: 1.239524Epoch 47 		 Validation Loss: 1.265623, 		 Training Loss: 1.236863Epoch 48 		 Validation Loss: 1.265591, 		 Training Loss: 1.254071Validation Loss Decreased(70.869323--->70.867245) Saving model ...
Epoch 49 		 Validation Loss: 1.265487, 		 Training Loss: 1.252712Epoch 50 		 Validation Loss: 1.265502, 		 Training Loss: 1.251560Validation Loss Decreased(70.867245--->70.862142) Saving model ...
Epoch 51 		 Validation Loss: 1.265395, 		 Training Loss: 1.237365Epoch 52 		 Validation Loss: 1.265402, 		 Training Loss: 1.242349Epoch 53 		 Validation Loss: 1.265406, 		 Training Loss: 1.242631Epoch 54 		 Validation Loss: 1.265404, 		 Training Loss: 1.246341Epoch 55 		 Validation Loss: 1.265433, 		 Training Loss: 1.245424Epoch 56 		 Validation Loss: 1.265422, 		 Training Loss: 1.244298Epoch 57 		 Validation Loss: 1.265403, 		 Training Loss: 1.237423Epoch 58 		 Validation Loss: 1.265410, 		 Training Loss: 1.247420Epoch 59 		 Validation Loss: 1.265415, 		 Training Loss: 1.246449Epoch 60 		 Validation Loss: 1.265403, 		 Training Loss: 1.240619Epoch 61 		 Validation Loss: 1.265398, 		 Training Loss: 1.243663Epoch 62 		 Validation Loss: 1.265398, 		 Training Loss: 1.237803Epoch 63 		 Validation Loss: 1.265423, 		 Training Loss: 1.236907Validation Loss Decreased(70.862142--->70.862024) Saving model ...
Epoch 64 		 Validation Loss: 1.265393, 		 Training Loss: 1.250369Epoch 65 		 Validation Loss: 1.265417, 		 Training Loss: 1.247733Epoch 66 		 Validation Loss: 1.265405, 		 Training Loss: 1.235164Epoch 67 		 Validation Loss: 1.265409, 		 Training Loss: 1.246039Validation Loss Decreased(70.862024--->70.861556) Saving model ...
Epoch 68 		 Validation Loss: 1.265385, 		 Training Loss: 1.242903Validation Loss Decreased(70.861556--->70.861318) Saving model ...
Epoch 69 		 Validation Loss: 1.265381, 		 Training Loss: 1.248983Validation Loss Decreased(70.861318--->70.860851) Saving model ...
Epoch 70 		 Validation Loss: 1.265372, 		 Training Loss: 1.246159Validation Loss Decreased(70.860851--->70.860040) Saving model ...
Epoch 71 		 Validation Loss: 1.265358, 		 Training Loss: 1.243554Validation Loss Decreased(70.860040--->70.859645) Saving model ...
Epoch 72 		 Validation Loss: 1.265351, 		 Training Loss: 1.247120Epoch 73 		 Validation Loss: 1.265352, 		 Training Loss: 1.248762Validation Loss Decreased(70.859645--->70.858488) Saving model ...
Epoch 74 		 Validation Loss: 1.265330, 		 Training Loss: 1.249662Validation Loss Decreased(70.858488--->70.858310) Saving model ...
Epoch 75 		 Validation Loss: 1.265327, 		 Training Loss: 1.245452Epoch 76 		 Validation Loss: 1.265338, 		 Training Loss: 1.247739Validation Loss Decreased(70.858310--->70.857776) Saving model ...
Epoch 77 		 Validation Loss: 1.265317, 		 Training Loss: 1.243379Epoch 78 		 Validation Loss: 1.265318, 		 Training Loss: 1.240338Epoch 79 		 Validation Loss: 1.265318, 		 Training Loss: 1.244673Validation Loss Decreased(70.857776--->70.856846) Saving model ...
Epoch 80 		 Validation Loss: 1.265301, 		 Training Loss: 1.241716Validation Loss Decreased(70.856846--->70.856791) Saving model ...
Epoch 81 		 Validation Loss: 1.265300, 		 Training Loss: 1.235772Epoch 82 		 Validation Loss: 1.265339, 		 Training Loss: 1.241218Epoch 83 		 Validation Loss: 1.265316, 		 Training Loss: 1.234572Epoch 84 		 Validation Loss: 1.265313, 		 Training Loss: 1.246506Epoch 85 		 Validation Loss: 1.265305, 		 Training Loss: 1.244855Epoch 86 		 Validation Loss: 1.265314, 		 Training Loss: 1.232315Epoch 87 		 Validation Loss: 1.265311, 		 Training Loss: 1.252217Validation Loss Decreased(70.856791--->70.856361) Saving model ...
Epoch 88 		 Validation Loss: 1.265292, 		 Training Loss: 1.257458Epoch 89 		 Validation Loss: 1.265329, 		 Training Loss: 1.237254Validation Loss Decreased(70.856361--->70.856163) Saving model ...
Epoch 90 		 Validation Loss: 1.265289, 		 Training Loss: 1.241785Epoch 91 		 Validation Loss: 1.265292, 		 Training Loss: 1.235132Validation Loss Decreased(70.856163--->70.855751) Saving model ...
Epoch 92 		 Validation Loss: 1.265281, 		 Training Loss: 1.249598Validation Loss Decreased(70.855751--->70.854425) Saving model ...
Epoch 93 		 Validation Loss: 1.265258, 		 Training Loss: 1.252708Validation Loss Decreased(70.854425--->70.853484) Saving model ...
Epoch 94 		 Validation Loss: 1.265241, 		 Training Loss: 1.250664Epoch 95 		 Validation Loss: 1.265258, 		 Training Loss: 1.235279Epoch 96 		 Validation Loss: 1.265260, 		 Training Loss: 1.232549Epoch 97 		 Validation Loss: 1.265254, 		 Training Loss: 1.249372Epoch 98 		 Validation Loss: 1.265258, 		 Training Loss: 1.245395Epoch 99 		 Validation Loss: 1.265260, 		 Training Loss: 1.243707Epoch 100 		 Validation Loss: 1.265266, 		 Training Loss: 1.238563Epoch 101 		 Validation Loss: 1.265268, 		 Training Loss: 1.250935Epoch 102 		 Validation Loss: 1.265251, 		 Training Loss: 1.245261Validation Loss Decreased(70.853484--->70.852759) Saving model ...
Epoch 103 		 Validation Loss: 1.265228, 		 Training Loss: 1.253229Epoch 104 		 Validation Loss: 1.265236, 		 Training Loss: 1.234026Validation Loss Decreased(70.852759--->70.851632) Saving model ...
Epoch 105 		 Validation Loss: 1.265208, 		 Training Loss: 1.252497Epoch 106 		 Validation Loss: 1.265220, 		 Training Loss: 1.232512Epoch 107 		 Validation Loss: 1.265242, 		 Training Loss: 1.240251Epoch 108 		 Validation Loss: 1.265259, 		 Training Loss: 1.246811Epoch 109 		 Validation Loss: 1.265210, 		 Training Loss: 1.246651Epoch 110 		 Validation Loss: 1.265230, 		 Training Loss: 1.240435Epoch 111 		 Validation Loss: 1.265219, 		 Training Loss: 1.239050Validation Loss Decreased(70.851632--->70.851299) Saving model ...
Epoch 112 		 Validation Loss: 1.265202, 		 Training Loss: 1.242163Validation Loss Decreased(70.851299--->70.850077) Saving model ...
Epoch 113 		 Validation Loss: 1.265180, 		 Training Loss: 1.251377Validation Loss Decreased(70.850077--->70.849245) Saving model ...
Epoch 114 		 Validation Loss: 1.265165, 		 Training Loss: 1.252997Epoch 115 		 Validation Loss: 1.265165, 		 Training Loss: 1.247908Validation Loss Decreased(70.849245--->70.848778) Saving model ...
Epoch 116 		 Validation Loss: 1.265157, 		 Training Loss: 1.238357Epoch 117 		 Validation Loss: 1.265172, 		 Training Loss: 1.244984Validation Loss Decreased(70.848778--->70.847788) Saving model ...
Epoch 118 		 Validation Loss: 1.265139, 		 Training Loss: 1.250443Epoch 119 		 Validation Loss: 1.265147, 		 Training Loss: 1.240558Validation Loss Decreased(70.847788--->70.847121) Saving model ...
Epoch 120 		 Validation Loss: 1.265127, 		 Training Loss: 1.241547Validation Loss Decreased(70.847121--->70.847073) Saving model ...
Epoch 121 		 Validation Loss: 1.265126, 		 Training Loss: 1.244383Epoch 122 		 Validation Loss: 1.265128, 		 Training Loss: 1.239323Validation Loss Decreased(70.847073--->70.846984) Saving model ...
Epoch 123 		 Validation Loss: 1.265125, 		 Training Loss: 1.236336Validation Loss Decreased(70.846984--->70.846839) Saving model ...
Epoch 124 		 Validation Loss: 1.265122, 		 Training Loss: 1.244797Validation Loss Decreased(70.846839--->70.845503) Saving model ...
Epoch 125 		 Validation Loss: 1.265098, 		 Training Loss: 1.245223Epoch 126 		 Validation Loss: 1.265103, 		 Training Loss: 1.240495Epoch 127 		 Validation Loss: 1.265108, 		 Training Loss: 1.233479Epoch 128 		 Validation Loss: 1.265104, 		 Training Loss: 1.257981Validation Loss Decreased(70.845503--->70.845289) Saving model ...
Epoch 129 		 Validation Loss: 1.265094, 		 Training Loss: 1.254195Validation Loss Decreased(70.845289--->70.844646) Saving model ...
Epoch 130 		 Validation Loss: 1.265083, 		 Training Loss: 1.248388Epoch 131 		 Validation Loss: 1.265094, 		 Training Loss: 1.239369Epoch 132 		 Validation Loss: 1.265086, 		 Training Loss: 1.240119Validation Loss Decreased(70.844646--->70.844335) Saving model ...
Epoch 133 		 Validation Loss: 1.265077, 		 Training Loss: 1.245909Validation Loss Decreased(70.844335--->70.844026) Saving model ...
Epoch 134 		 Validation Loss: 1.265072, 		 Training Loss: 1.250166Validation Loss Decreased(70.844026--->70.844012) Saving model ...
Epoch 135 		 Validation Loss: 1.265072, 		 Training Loss: 1.239423Validation Loss Decreased(70.844012--->70.843783) Saving model ...
Epoch 136 		 Validation Loss: 1.265068, 		 Training Loss: 1.248927Validation Loss Decreased(70.843783--->70.843114) Saving model ...
Epoch 137 		 Validation Loss: 1.265056, 		 Training Loss: 1.244289Epoch 138 		 Validation Loss: 1.265096, 		 Training Loss: 1.257372Epoch 139 		 Validation Loss: 1.265083, 		 Training Loss: 1.242529Validation Loss Decreased(70.843114--->70.842135) Saving model ...
Epoch 140 		 Validation Loss: 1.265038, 		 Training Loss: 1.242239Epoch 141 		 Validation Loss: 1.265038, 		 Training Loss: 1.236031Validation Loss Decreased(70.842135--->70.841918) Saving model ...
Epoch 142 		 Validation Loss: 1.265034, 		 Training Loss: 1.235066Epoch 143 		 Validation Loss: 1.265054, 		 Training Loss: 1.240753Validation Loss Decreased(70.841918--->70.841298) Saving model ...
Epoch 144 		 Validation Loss: 1.265023, 		 Training Loss: 1.258222Epoch 145 		 Validation Loss: 1.265048, 		 Training Loss: 1.232352Epoch 146 		 Validation Loss: 1.265045, 		 Training Loss: 1.235460Epoch 147 		 Validation Loss: 1.265034, 		 Training Loss: 1.239965Validation Loss Decreased(70.841298--->70.840703) Saving model ...
Epoch 148 		 Validation Loss: 1.265013, 		 Training Loss: 1.258539Epoch 149 		 Validation Loss: 1.265053, 		 Training Loss: 1.250933Epoch 150 		 Validation Loss: 1.265018, 		 Training Loss: 1.237536Epoch 151 		 Validation Loss: 1.265013, 		 Training Loss: 1.241694Epoch 152 		 Validation Loss: 1.265017, 		 Training Loss: 1.246451Validation Loss Decreased(70.840703--->70.840013) Saving model ...
Epoch 153 		 Validation Loss: 1.265000, 		 Training Loss: 1.256171Validation Loss Decreased(70.840013--->70.838988) Saving model ...
Epoch 154 		 Validation Loss: 1.264982, 		 Training Loss: 1.256693Validation Loss Decreased(70.838988--->70.837899) Saving model ...
Epoch 155 		 Validation Loss: 1.264962, 		 Training Loss: 1.240770Epoch 156 		 Validation Loss: 1.264988, 		 Training Loss: 1.245871Epoch 157 		 Validation Loss: 1.264968, 		 Training Loss: 1.237612Epoch 158 		 Validation Loss: 1.264967, 		 Training Loss: 1.242256Epoch 159 		 Validation Loss: 1.265008, 		 Training Loss: 1.249723Epoch 160 		 Validation Loss: 1.264999, 		 Training Loss: 1.244381Validation Loss Decreased(70.837899--->70.836057) Saving model ...
Epoch 161 		 Validation Loss: 1.264930, 		 Training Loss: 1.242740Epoch 162 		 Validation Loss: 1.264930, 		 Training Loss: 1.256648Validation Loss Decreased(70.836057--->70.835945) Saving model ...
Epoch 163 		 Validation Loss: 1.264928, 		 Training Loss: 1.242151Validation Loss Decreased(70.835945--->70.835160) Saving model ...
Epoch 164 		 Validation Loss: 1.264914, 		 Training Loss: 1.240193Epoch 165 		 Validation Loss: 1.264923, 		 Training Loss: 1.240220Epoch 166 		 Validation Loss: 1.264918, 		 Training Loss: 1.240754Epoch 167 		 Validation Loss: 1.264922, 		 Training Loss: 1.239337Epoch 168 		 Validation Loss: 1.264973, 		 Training Loss: 1.236554Epoch 169 		 Validation Loss: 1.264966, 		 Training Loss: 1.235923Epoch 170 		 Validation Loss: 1.264955, 		 Training Loss: 1.236072Epoch 171 		 Validation Loss: 1.264941, 		 Training Loss: 1.237234Epoch 172 		 Validation Loss: 1.264952, 		 Training Loss: 1.238350Epoch 173 		 Validation Loss: 1.264922, 		 Training Loss: 1.259632Epoch 174 		 Validation Loss: 1.264927, 		 Training Loss: 1.244019Epoch 175 		 Validation Loss: 1.264921, 		 Training Loss: 1.236756Epoch 176 		 Validation Loss: 1.264918, 		 Training Loss: 1.248018Validation Loss Decreased(70.835160--->70.834914) Saving model ...
Epoch 177 		 Validation Loss: 1.264909, 		 Training Loss: 1.257916Validation Loss Decreased(70.834914--->70.832912) Saving model ...
Epoch 178 		 Validation Loss: 1.264873, 		 Training Loss: 1.264606Epoch 179 		 Validation Loss: 1.264913, 		 Training Loss: 1.240137Validation Loss Decreased(70.832912--->70.832835) Saving model ...
Epoch 180 		 Validation Loss: 1.264872, 		 Training Loss: 1.239082Epoch 181 		 Validation Loss: 1.264875, 		 Training Loss: 1.238809Validation Loss Decreased(70.832835--->70.832017) Saving model ...
Epoch 182 		 Validation Loss: 1.264857, 		 Training Loss: 1.246520Epoch 183 		 Validation Loss: 1.264858, 		 Training Loss: 1.238655Epoch 184 		 Validation Loss: 1.264869, 		 Training Loss: 1.247370Validation Loss Decreased(70.832017--->70.830780) Saving model ...
Epoch 185 		 Validation Loss: 1.264835, 		 Training Loss: 1.248106Epoch 186 		 Validation Loss: 1.264845, 		 Training Loss: 1.245977Validation Loss Decreased(70.830780--->70.830107) Saving model ...
Epoch 187 		 Validation Loss: 1.264823, 		 Training Loss: 1.246722Validation Loss Decreased(70.830107--->70.829783) Saving model ...
Epoch 188 		 Validation Loss: 1.264818, 		 Training Loss: 1.247646Epoch 189 		 Validation Loss: 1.264821, 		 Training Loss: 1.236322Epoch 190 		 Validation Loss: 1.264837, 		 Training Loss: 1.239243Epoch 191 		 Validation Loss: 1.264853, 		 Training Loss: 1.241329Epoch 192 		 Validation Loss: 1.264825, 		 Training Loss: 1.237758Epoch 193 		 Validation Loss: 1.264822, 		 Training Loss: 1.245914Validation Loss Decreased(70.829783--->70.827959) Saving model ...
Epoch 194 		 Validation Loss: 1.264785, 		 Training Loss: 1.265763Epoch 195 		 Validation Loss: 1.264811, 		 Training Loss: 1.233421Epoch 196 		 Validation Loss: 1.264808, 		 Training Loss: 1.241356Epoch 197 		 Validation Loss: 1.264805, 		 Training Loss: 1.237691Validation Loss Decreased(70.827959--->70.827188) Saving model ...
Epoch 198 		 Validation Loss: 1.264771, 		 Training Loss: 1.248123Epoch 199 		 Validation Loss: 1.264829, 		 Training Loss: 1.239982Validation Loss Decreased(70.827188--->70.826287) Saving model ...
Epoch 200 		 Validation Loss: 1.264755, 		 Training Loss: 1.243900Epoch 201 		 Validation Loss: 1.264771, 		 Training Loss: 1.241672Validation Loss Decreased(70.826287--->70.826229) Saving model ...
Epoch 202 		 Validation Loss: 1.264754, 		 Training Loss: 1.245482Epoch 203 		 Validation Loss: 1.264771, 		 Training Loss: 1.244197Validation Loss Decreased(70.826229--->70.825733) Saving model ...
Epoch 204 		 Validation Loss: 1.264745, 		 Training Loss: 1.239427Epoch 205 		 Validation Loss: 1.264758, 		 Training Loss: 1.245977Epoch 206 		 Validation Loss: 1.264758, 		 Training Loss: 1.246328Epoch 207 		 Validation Loss: 1.264774, 		 Training Loss: 1.229433Validation Loss Decreased(70.825733--->70.824771) Saving model ...
Epoch 208 		 Validation Loss: 1.264728, 		 Training Loss: 1.241862Epoch 209 		 Validation Loss: 1.264774, 		 Training Loss: 1.228802Epoch 210 		 Validation Loss: 1.264741, 		 Training Loss: 1.242332Epoch 211 		 Validation Loss: 1.264745, 		 Training Loss: 1.237125Epoch 212 		 Validation Loss: 1.264752, 		 Training Loss: 1.244703Epoch 213 		 Validation Loss: 1.264733, 		 Training Loss: 1.245594Validation Loss Decreased(70.824771--->70.822545) Saving model ...
Epoch 214 		 Validation Loss: 1.264688, 		 Training Loss: 1.246695Epoch 215 		 Validation Loss: 1.264691, 		 Training Loss: 1.241782Epoch 216 		 Validation Loss: 1.264697, 		 Training Loss: 1.244491Epoch 217 		 Validation Loss: 1.264694, 		 Training Loss: 1.230499Epoch 218 		 Validation Loss: 1.264743, 		 Training Loss: 1.240043Epoch 219 		 Validation Loss: 1.264701, 		 Training Loss: 1.241051Epoch 220 		 Validation Loss: 1.264732, 		 Training Loss: 1.239684Epoch 221 		 Validation Loss: 1.264691, 		 Training Loss: 1.240079Epoch 222 		 Validation Loss: 1.264695, 		 Training Loss: 1.257428Validation Loss Decreased(70.822545--->70.820937) Saving model ...
Epoch 223 		 Validation Loss: 1.264660, 		 Training Loss: 1.242639Validation Loss Decreased(70.820937--->70.819727) Saving model ...
Epoch 224 		 Validation Loss: 1.264638, 		 Training Loss: 1.246673Epoch 225 		 Validation Loss: 1.264668, 		 Training Loss: 1.236167Validation Loss Decreased(70.819727--->70.818646) Saving model ...
Epoch 226 		 Validation Loss: 1.264619, 		 Training Loss: 1.254228Validation Loss Decreased(70.818646--->70.817621) Saving model ...
Epoch 227 		 Validation Loss: 1.264600, 		 Training Loss: 1.246004Validation Loss Decreased(70.817621--->70.816673) Saving model ...
Epoch 228 		 Validation Loss: 1.264583, 		 Training Loss: 1.242875Epoch 229 		 Validation Loss: 1.264603, 		 Training Loss: 1.242883Validation Loss Decreased(70.816673--->70.815550) Saving model ...
Epoch 230 		 Validation Loss: 1.264563, 		 Training Loss: 1.242765Epoch 231 		 Validation Loss: 1.264573, 		 Training Loss: 1.232593Epoch 232 		 Validation Loss: 1.264624, 		 Training Loss: 1.252627Epoch 233 		 Validation Loss: 1.264564, 		 Training Loss: 1.241081Validation Loss Decreased(70.815550--->70.815547) Saving model ...
Epoch 234 		 Validation Loss: 1.264563, 		 Training Loss: 1.241007Epoch 235 		 Validation Loss: 1.264571, 		 Training Loss: 1.233460Validation Loss Decreased(70.815547--->70.814899) Saving model ...
Epoch 236 		 Validation Loss: 1.264552, 		 Training Loss: 1.241939Validation Loss Decreased(70.814899--->70.813822) Saving model ...
Epoch 237 		 Validation Loss: 1.264533, 		 Training Loss: 1.249455Epoch 238 		 Validation Loss: 1.264544, 		 Training Loss: 1.231776Epoch 239 		 Validation Loss: 1.264597, 		 Training Loss: 1.232852Epoch 240 		 Validation Loss: 1.264572, 		 Training Loss: 1.245124Epoch 241 		 Validation Loss: 1.264571, 		 Training Loss: 1.243460Validation Loss Decreased(70.813822--->70.812414) Saving model ...
Epoch 242 		 Validation Loss: 1.264507, 		 Training Loss: 1.241431Validation Loss Decreased(70.812414--->70.812187) Saving model ...
Epoch 243 		 Validation Loss: 1.264503, 		 Training Loss: 1.239902Epoch 244 		 Validation Loss: 1.264510, 		 Training Loss: 1.231165Epoch 245 		 Validation Loss: 1.264542, 		 Training Loss: 1.251428Validation Loss Decreased(70.812187--->70.810290) Saving model ...
Epoch 246 		 Validation Loss: 1.264469, 		 Training Loss: 1.257553Epoch 247 		 Validation Loss: 1.264481, 		 Training Loss: 1.236262Epoch 248 		 Validation Loss: 1.264497, 		 Training Loss: 1.247231Validation Loss Decreased(70.810290--->70.809992) Saving model ...
Epoch 249 		 Validation Loss: 1.264464, 		 Training Loss: 1.238431Epoch 250 		 Validation Loss: 1.264499, 		 Training Loss: 1.239510Epoch 251 		 Validation Loss: 1.264474, 		 Training Loss: 1.230703Epoch 252 		 Validation Loss: 1.264497, 		 Training Loss: 1.241167Epoch 253 		 Validation Loss: 1.264471, 		 Training Loss: 1.244330Epoch 254 		 Validation Loss: 1.264466, 		 Training Loss: 1.248210Validation Loss Decreased(70.809992--->70.809542) Saving model ...
Epoch 255 		 Validation Loss: 1.264456, 		 Training Loss: 1.243697Validation Loss Decreased(70.809542--->70.809084) Saving model ...
Epoch 256 		 Validation Loss: 1.264448, 		 Training Loss: 1.237040Epoch 257 		 Validation Loss: 1.264461, 		 Training Loss: 1.233489Epoch 258 		 Validation Loss: 1.264462, 		 Training Loss: 1.238911Epoch 259 		 Validation Loss: 1.264470, 		 Training Loss: 1.240202Epoch 260 		 Validation Loss: 1.264455, 		 Training Loss: 1.239363Epoch 261 		 Validation Loss: 1.264459, 		 Training Loss: 1.246622Epoch 262 		 Validation Loss: 1.264459, 		 Training Loss: 1.241836Epoch 263 		 Validation Loss: 1.264504, 		 Training Loss: 1.233761Epoch 264 		 Validation Loss: 1.264476, 		 Training Loss: 1.241946Epoch 265 		 Validation Loss: 1.264448, 		 Training Loss: 1.240702Validation Loss Decreased(70.809084--->70.808742) Saving model ...
Epoch 266 		 Validation Loss: 1.264442, 		 Training Loss: 1.235888Validation Loss Decreased(70.808742--->70.807434) Saving model ...
Epoch 267 		 Validation Loss: 1.264418, 		 Training Loss: 1.240840Epoch 268 		 Validation Loss: 1.264470, 		 Training Loss: 1.237313Epoch 269 		 Validation Loss: 1.264434, 		 Training Loss: 1.241101Validation Loss Decreased(70.807434--->70.807289) Saving model ...
Epoch 270 		 Validation Loss: 1.264416, 		 Training Loss: 1.238083Epoch 271 		 Validation Loss: 1.264424, 		 Training Loss: 1.243739Validation Loss Decreased(70.807289--->70.807215) Saving model ...
Epoch 272 		 Validation Loss: 1.264415, 		 Training Loss: 1.248096Epoch 273 		 Validation Loss: 1.264463, 		 Training Loss: 1.243287Validation Loss Decreased(70.807215--->70.804879) Saving model ...
Epoch 274 		 Validation Loss: 1.264373, 		 Training Loss: 1.240238Epoch 275 		 Validation Loss: 1.264374, 		 Training Loss: 1.241339Validation Loss Decreased(70.804879--->70.804736) Saving model ...
Epoch 276 		 Validation Loss: 1.264370, 		 Training Loss: 1.239369Epoch 277 		 Validation Loss: 1.264401, 		 Training Loss: 1.239507Epoch 278 		 Validation Loss: 1.264430, 		 Training Loss: 1.243613Epoch 279 		 Validation Loss: 1.264393, 		 Training Loss: 1.239922Epoch 280 		 Validation Loss: 1.264380, 		 Training Loss: 1.230162Epoch 281 		 Validation Loss: 1.264398, 		 Training Loss: 1.236412Epoch 282 		 Validation Loss: 1.264396, 		 Training Loss: 1.246300Validation Loss Decreased(70.804736--->70.802985) Saving model ...
Epoch 283 		 Validation Loss: 1.264339, 		 Training Loss: 1.248336Validation Loss Decreased(70.802985--->70.802359) Saving model ...
Epoch 284 		 Validation Loss: 1.264328, 		 Training Loss: 1.244048Validation Loss Decreased(70.802359--->70.801923) Saving model ...
Epoch 285 		 Validation Loss: 1.264320, 		 Training Loss: 1.253941Epoch 286 		 Validation Loss: 1.264325, 		 Training Loss: 1.231005Epoch 287 		 Validation Loss: 1.264361, 		 Training Loss: 1.235378Epoch 288 		 Validation Loss: 1.264324, 		 Training Loss: 1.238123Epoch 289 		 Validation Loss: 1.264332, 		 Training Loss: 1.230759Validation Loss Decreased(70.801923--->70.801259) Saving model ...
Epoch 290 		 Validation Loss: 1.264308, 		 Training Loss: 1.250809Validation Loss Decreased(70.801259--->70.800380) Saving model ...
Epoch 291 		 Validation Loss: 1.264293, 		 Training Loss: 1.240466Validation Loss Decreased(70.800380--->70.799842) Saving model ...
Epoch 292 		 Validation Loss: 1.264283, 		 Training Loss: 1.239938Epoch 293 		 Validation Loss: 1.264336, 		 Training Loss: 1.241342Epoch 294 		 Validation Loss: 1.264365, 		 Training Loss: 1.236546Validation Loss Decreased(70.799842--->70.798500) Saving model ...
Epoch 295 		 Validation Loss: 1.264259, 		 Training Loss: 1.250108Epoch 296 		 Validation Loss: 1.264264, 		 Training Loss: 1.241928Epoch 297 		 Validation Loss: 1.264272, 		 Training Loss: 1.245142Validation Loss Decreased(70.798500--->70.796129) Saving model ...
Epoch 298 		 Validation Loss: 1.264217, 		 Training Loss: 1.255667Epoch 299 		 Validation Loss: 1.264227, 		 Training Loss: 1.245563Validation Loss Decreased(70.796129--->70.794991) Saving model ...
Epoch 300 		 Validation Loss: 1.264196, 		 Training Loss: 1.242345Epoch 301 		 Validation Loss: 1.264208, 		 Training Loss: 1.236709Epoch 302 		 Validation Loss: 1.264196, 		 Training Loss: 1.248621Validation Loss Decreased(70.794991--->70.794113) Saving model ...
Epoch 303 		 Validation Loss: 1.264181, 		 Training Loss: 1.242984Epoch 304 		 Validation Loss: 1.264191, 		 Training Loss: 1.251840Validation Loss Decreased(70.794113--->70.792275) Saving model ...
Epoch 305 		 Validation Loss: 1.264148, 		 Training Loss: 1.247094Validation Loss Decreased(70.792275--->70.791841) Saving model ...
Epoch 306 		 Validation Loss: 1.264140, 		 Training Loss: 1.246558Epoch 307 		 Validation Loss: 1.264171, 		 Training Loss: 1.259306Validation Loss Decreased(70.791841--->70.789830) Saving model ...
Epoch 308 		 Validation Loss: 1.264104, 		 Training Loss: 1.248768Epoch 309 		 Validation Loss: 1.264122, 		 Training Loss: 1.234431Validation Loss Decreased(70.789830--->70.788503) Saving model ...
Epoch 310 		 Validation Loss: 1.264080, 		 Training Loss: 1.252995Validation Loss Decreased(70.788503--->70.787644) Saving model ...
Epoch 311 		 Validation Loss: 1.264065, 		 Training Loss: 1.244928Validation Loss Decreased(70.787644--->70.787149) Saving model ...
Epoch 312 		 Validation Loss: 1.264056, 		 Training Loss: 1.241720Epoch 313 		 Validation Loss: 1.264086, 		 Training Loss: 1.240546Epoch 314 		 Validation Loss: 1.264057, 		 Training Loss: 1.234032Validation Loss Decreased(70.787149--->70.787117) Saving model ...
Epoch 315 		 Validation Loss: 1.264056, 		 Training Loss: 1.246284Validation Loss Decreased(70.787117--->70.786663) Saving model ...
Epoch 316 		 Validation Loss: 1.264048, 		 Training Loss: 1.240848Epoch 317 		 Validation Loss: 1.264061, 		 Training Loss: 1.245009Epoch 318 		 Validation Loss: 1.264049, 		 Training Loss: 1.249785Validation Loss Decreased(70.786663--->70.786198) Saving model ...
Epoch 319 		 Validation Loss: 1.264039, 		 Training Loss: 1.240800Validation Loss Decreased(70.786198--->70.784668) Saving model ...
Epoch 320 		 Validation Loss: 1.264012, 		 Training Loss: 1.238518Validation Loss Decreased(70.784668--->70.784435) Saving model ...
Epoch 321 		 Validation Loss: 1.264008, 		 Training Loss: 1.236983Validation Loss Decreased(70.784435--->70.783819) Saving model ...
Epoch 322 		 Validation Loss: 1.263997, 		 Training Loss: 1.247055Epoch 323 		 Validation Loss: 1.264013, 		 Training Loss: 1.241208Validation Loss Decreased(70.783819--->70.783535) Saving model ...
Epoch 324 		 Validation Loss: 1.263992, 		 Training Loss: 1.237679Validation Loss Decreased(70.783535--->70.783338) Saving model ...
Epoch 325 		 Validation Loss: 1.263988, 		 Training Loss: 1.257611Validation Loss Decreased(70.783338--->70.782375) Saving model ...
Epoch 326 		 Validation Loss: 1.263971, 		 Training Loss: 1.246037Epoch 327 		 Validation Loss: 1.263986, 		 Training Loss: 1.232706Validation Loss Decreased(70.782375--->70.781099) Saving model ...
Epoch 328 		 Validation Loss: 1.263948, 		 Training Loss: 1.253510Validation Loss Decreased(70.781099--->70.780879) Saving model ...
Epoch 329 		 Validation Loss: 1.263944, 		 Training Loss: 1.239293Validation Loss Decreased(70.780879--->70.780173) Saving model ...
Epoch 330 		 Validation Loss: 1.263932, 		 Training Loss: 1.255965Epoch 331 		 Validation Loss: 1.263955, 		 Training Loss: 1.256927Validation Loss Decreased(70.780173--->70.779384) Saving model ...
Epoch 332 		 Validation Loss: 1.263918, 		 Training Loss: 1.238589Validation Loss Decreased(70.779384--->70.778677) Saving model ...
Epoch 333 		 Validation Loss: 1.263905, 		 Training Loss: 1.245612Epoch 334 		 Validation Loss: 1.263959, 		 Training Loss: 1.235191Epoch 335 		 Validation Loss: 1.263966, 		 Training Loss: 1.237274Epoch 336 		 Validation Loss: 1.263994, 		 Training Loss: 1.234630Epoch 337 		 Validation Loss: 1.263952, 		 Training Loss: 1.242072Epoch 338 		 Validation Loss: 1.263956, 		 Training Loss: 1.239214Epoch 339 		 Validation Loss: 1.263945, 		 Training Loss: 1.245639Epoch 340 		 Validation Loss: 1.263946, 		 Training Loss: 1.240622Epoch 341 		 Validation Loss: 1.263951, 		 Training Loss: 1.240857Validation Loss Decreased(70.778677--->70.778509) Saving model ...
Epoch 342 		 Validation Loss: 1.263902, 		 Training Loss: 1.247407Validation Loss Decreased(70.778509--->70.777438) Saving model ...
Epoch 343 		 Validation Loss: 1.263883, 		 Training Loss: 1.250235Validation Loss Decreased(70.777438--->70.777313) Saving model ...
Epoch 344 		 Validation Loss: 1.263881, 		 Training Loss: 1.235484Epoch 345 		 Validation Loss: 1.263906, 		 Training Loss: 1.242529Validation Loss Decreased(70.777313--->70.775953) Saving model ...
Epoch 346 		 Validation Loss: 1.263856, 		 Training Loss: 1.232571Epoch 347 		 Validation Loss: 1.263861, 		 Training Loss: 1.242571Epoch 348 		 Validation Loss: 1.263871, 		 Training Loss: 1.246586Validation Loss Decreased(70.775953--->70.775119) Saving model ...
Epoch 349 		 Validation Loss: 1.263841, 		 Training Loss: 1.242875Validation Loss Decreased(70.775119--->70.774821) Saving model ...
Epoch 350 		 Validation Loss: 1.263836, 		 Training Loss: 1.247073Validation Loss Decreased(70.774821--->70.774368) Saving model ...
Epoch 351 		 Validation Loss: 1.263828, 		 Training Loss: 1.240024Validation Loss Decreased(70.774368--->70.773020) Saving model ...
Epoch 352 		 Validation Loss: 1.263804, 		 Training Loss: 1.255023Epoch 353 		 Validation Loss: 1.263825, 		 Training Loss: 1.232752Epoch 354 		 Validation Loss: 1.263816, 		 Training Loss: 1.240389Epoch 355 		 Validation Loss: 1.263825, 		 Training Loss: 1.240370Validation Loss Decreased(70.773020--->70.772301) Saving model ...
Epoch 356 		 Validation Loss: 1.263791, 		 Training Loss: 1.242260Epoch 357 		 Validation Loss: 1.263813, 		 Training Loss: 1.240673Validation Loss Decreased(70.772301--->70.771156) Saving model ...
Epoch 358 		 Validation Loss: 1.263771, 		 Training Loss: 1.247402Validation Loss Decreased(70.771156--->70.770252) Saving model ...
Epoch 359 		 Validation Loss: 1.263755, 		 Training Loss: 1.244904Epoch 360 		 Validation Loss: 1.263756, 		 Training Loss: 1.243002Validation Loss Decreased(70.770252--->70.769325) Saving model ...
Epoch 361 		 Validation Loss: 1.263738, 		 Training Loss: 1.251216Epoch 362 		 Validation Loss: 1.263797, 		 Training Loss: 1.241205Validation Loss Decreased(70.769325--->70.768608) Saving model ...
Epoch 363 		 Validation Loss: 1.263725, 		 Training Loss: 1.246180Epoch 364 		 Validation Loss: 1.263791, 		 Training Loss: 1.235271Epoch 365 		 Validation Loss: 1.263763, 		 Training Loss: 1.232810Epoch 366 		 Validation Loss: 1.263736, 		 Training Loss: 1.247196Epoch 367 		 Validation Loss: 1.263732, 		 Training Loss: 1.240848Epoch 368 		 Validation Loss: 1.263742, 		 Training Loss: 1.246058Validation Loss Decreased(70.768608--->70.768109) Saving model ...
Epoch 369 		 Validation Loss: 1.263716, 		 Training Loss: 1.247579Epoch 370 		 Validation Loss: 1.263754, 		 Training Loss: 1.237756Validation Loss Decreased(70.768109--->70.767004) Saving model ...
Epoch 371 		 Validation Loss: 1.263696, 		 Training Loss: 1.245925Epoch 372 		 Validation Loss: 1.263697, 		 Training Loss: 1.238629Epoch 373 		 Validation Loss: 1.263701, 		 Training Loss: 1.236855Validation Loss Decreased(70.767004--->70.766281) Saving model ...
Epoch 374 		 Validation Loss: 1.263684, 		 Training Loss: 1.241878Epoch 375 		 Validation Loss: 1.263691, 		 Training Loss: 1.232922Epoch 376 		 Validation Loss: 1.263719, 		 Training Loss: 1.233475Epoch 377 		 Validation Loss: 1.263702, 		 Training Loss: 1.244004Validation Loss Decreased(70.766281--->70.766027) Saving model ...
Epoch 378 		 Validation Loss: 1.263679, 		 Training Loss: 1.244865Validation Loss Decreased(70.766027--->70.764476) Saving model ...
Epoch 379 		 Validation Loss: 1.263651, 		 Training Loss: 1.256506Epoch 380 		 Validation Loss: 1.263671, 		 Training Loss: 1.251139Validation Loss Decreased(70.764476--->70.763459) Saving model ...
Epoch 381 		 Validation Loss: 1.263633, 		 Training Loss: 1.231354Epoch 382 		 Validation Loss: 1.263676, 		 Training Loss: 1.239565Epoch 383 		 Validation Loss: 1.263657, 		 Training Loss: 1.233879Epoch 384 		 Validation Loss: 1.263647, 		 Training Loss: 1.234853Validation Loss Decreased(70.763459--->70.762947) Saving model ...
Epoch 385 		 Validation Loss: 1.263624, 		 Training Loss: 1.244561Epoch 386 		 Validation Loss: 1.263664, 		 Training Loss: 1.229629Validation Loss Decreased(70.762947--->70.762710) Saving model ...
Epoch 387 		 Validation Loss: 1.263620, 		 Training Loss: 1.241184Epoch 388 		 Validation Loss: 1.263648, 		 Training Loss: 1.236938Epoch 389 		 Validation Loss: 1.263623, 		 Training Loss: 1.238023Validation Loss Decreased(70.762710--->70.762284) Saving model ...
Epoch 390 		 Validation Loss: 1.263612, 		 Training Loss: 1.245702Validation Loss Decreased(70.762284--->70.761660) Saving model ...
Epoch 391 		 Validation Loss: 1.263601, 		 Training Loss: 1.256957Epoch 392 		 Validation Loss: 1.263625, 		 Training Loss: 1.256173Validation Loss Decreased(70.761660--->70.758334) Saving model ...
Epoch 393 		 Validation Loss: 1.263542, 		 Training Loss: 1.249521Validation Loss Decreased(70.758334--->70.758306) Saving model ...
Epoch 394 		 Validation Loss: 1.263541, 		 Training Loss: 1.237463Epoch 395 		 Validation Loss: 1.263543, 		 Training Loss: 1.235835Epoch 396 		 Validation Loss: 1.263545, 		 Training Loss: 1.226817Epoch 397 		 Validation Loss: 1.263574, 		 Training Loss: 1.244162Validation Loss Decreased(70.758306--->70.758088) Saving model ...
Epoch 398 		 Validation Loss: 1.263537, 		 Training Loss: 1.237926Validation Loss Decreased(70.758088--->70.757956) Saving model ...
Epoch 399 		 Validation Loss: 1.263535, 		 Training Loss: 1.238267Validation Loss Decreased(70.757956--->70.756515) Saving model ...
Epoch 400 		 Validation Loss: 1.263509, 		 Training Loss: 1.246029Validation Loss Decreased(70.756515--->70.756396) Saving model ...
Epoch 401 		 Validation Loss: 1.263507, 		 Training Loss: 1.243132Epoch 402 		 Validation Loss: 1.263517, 		 Training Loss: 1.250203Validation Loss Decreased(70.756396--->70.754998) Saving model ...
Epoch 403 		 Validation Loss: 1.263482, 		 Training Loss: 1.237018Epoch 404 		 Validation Loss: 1.263492, 		 Training Loss: 1.242094Validation Loss Decreased(70.754998--->70.754523) Saving model ...
Epoch 405 		 Validation Loss: 1.263474, 		 Training Loss: 1.246496Epoch 406 		 Validation Loss: 1.263487, 		 Training Loss: 1.236363Validation Loss Decreased(70.754523--->70.754381) Saving model ...
Epoch 407 		 Validation Loss: 1.263471, 		 Training Loss: 1.247833Validation Loss Decreased(70.754381--->70.751644) Saving model ...
Epoch 408 		 Validation Loss: 1.263422, 		 Training Loss: 1.252262Epoch 409 		 Validation Loss: 1.263442, 		 Training Loss: 1.247952Validation Loss Decreased(70.751644--->70.750502) Saving model ...
Epoch 410 		 Validation Loss: 1.263402, 		 Training Loss: 1.242645Validation Loss Decreased(70.750502--->70.749324) Saving model ...
Epoch 411 		 Validation Loss: 1.263381, 		 Training Loss: 1.240068Epoch 412 		 Validation Loss: 1.263389, 		 Training Loss: 1.239137Epoch 413 		 Validation Loss: 1.263395, 		 Training Loss: 1.242797Epoch 414 		 Validation Loss: 1.263392, 		 Training Loss: 1.233863Epoch 415 		 Validation Loss: 1.263404, 		 Training Loss: 1.232915Epoch 416 		 Validation Loss: 1.263401, 		 Training Loss: 1.236514Epoch 417 		 Validation Loss: 1.263407, 		 Training Loss: 1.239068Epoch 418 		 Validation Loss: 1.263403, 		 Training Loss: 1.234702Epoch 419 		 Validation Loss: 1.263389, 		 Training Loss: 1.239610Epoch 420 		 Validation Loss: 1.263390, 		 Training Loss: 1.232479Validation Loss Decreased(70.749324--->70.749290) Saving model ...
Epoch 421 		 Validation Loss: 1.263380, 		 Training Loss: 1.247854Epoch 422 		 Validation Loss: 1.263387, 		 Training Loss: 1.243443Validation Loss Decreased(70.749290--->70.748122) Saving model ...
Epoch 423 		 Validation Loss: 1.263359, 		 Training Loss: 1.245941Validation Loss Decreased(70.748122--->70.745665) Saving model ...
Epoch 424 		 Validation Loss: 1.263315, 		 Training Loss: 1.245957Epoch 425 		 Validation Loss: 1.263379, 		 Training Loss: 1.241661Epoch 426 		 Validation Loss: 1.263326, 		 Training Loss: 1.245035Validation Loss Decreased(70.745665--->70.743976) Saving model ...
Epoch 427 		 Validation Loss: 1.263285, 		 Training Loss: 1.254445Validation Loss Decreased(70.743976--->70.743786) Saving model ...
Epoch 428 		 Validation Loss: 1.263282, 		 Training Loss: 1.244637Validation Loss Decreased(70.743786--->70.743398) Saving model ...
Epoch 429 		 Validation Loss: 1.263275, 		 Training Loss: 1.235441Epoch 430 		 Validation Loss: 1.263277, 		 Training Loss: 1.245971Epoch 431 		 Validation Loss: 1.263286, 		 Training Loss: 1.248023Epoch 432 		 Validation Loss: 1.263326, 		 Training Loss: 1.247504Validation Loss Decreased(70.743398--->70.741238) Saving model ...
Epoch 433 		 Validation Loss: 1.263236, 		 Training Loss: 1.236873Validation Loss Decreased(70.741238--->70.741187) Saving model ...
Epoch 434 		 Validation Loss: 1.263235, 		 Training Loss: 1.239962Epoch 435 		 Validation Loss: 1.263250, 		 Training Loss: 1.237612Validation Loss Decreased(70.741187--->70.740597) Saving model ...
Epoch 436 		 Validation Loss: 1.263225, 		 Training Loss: 1.249559Validation Loss Decreased(70.740597--->70.739604) Saving model ...
Epoch 437 		 Validation Loss: 1.263207, 		 Training Loss: 1.247130Epoch 438 		 Validation Loss: 1.263210, 		 Training Loss: 1.244450Epoch 439 		 Validation Loss: 1.263209, 		 Training Loss: 1.242156Epoch 440 		 Validation Loss: 1.263265, 		 Training Loss: 1.240644Validation Loss Decreased(70.739604--->70.738420) Saving model ...
Epoch 441 		 Validation Loss: 1.263186, 		 Training Loss: 1.245306Epoch 442 		 Validation Loss: 1.263236, 		 Training Loss: 1.237474Epoch 443 		 Validation Loss: 1.263193, 		 Training Loss: 1.235621Validation Loss Decreased(70.738420--->70.738060) Saving model ...
Epoch 444 		 Validation Loss: 1.263180, 		 Training Loss: 1.241073Epoch 445 		 Validation Loss: 1.263202, 		 Training Loss: 1.234100Validation Loss Decreased(70.738060--->70.736267) Saving model ...
Epoch 446 		 Validation Loss: 1.263148, 		 Training Loss: 1.248612Epoch 447 		 Validation Loss: 1.263166, 		 Training Loss: 1.247594Validation Loss Decreased(70.736267--->70.735802) Saving model ...
Epoch 448 		 Validation Loss: 1.263139, 		 Training Loss: 1.243259Validation Loss Decreased(70.735802--->70.735798) Saving model ...
Epoch 449 		 Validation Loss: 1.263139, 		 Training Loss: 1.236441Epoch 450 		 Validation Loss: 1.263147, 		 Training Loss: 1.236595Epoch 451 		 Validation Loss: 1.263142, 		 Training Loss: 1.233291Validation Loss Decreased(70.735798--->70.735139) Saving model ...
Epoch 452 		 Validation Loss: 1.263127, 		 Training Loss: 1.255583Validation Loss Decreased(70.735139--->70.734936) Saving model ...
Epoch 453 		 Validation Loss: 1.263124, 		 Training Loss: 1.236904Epoch 454 		 Validation Loss: 1.263155, 		 Training Loss: 1.234468Epoch 455 		 Validation Loss: 1.263150, 		 Training Loss: 1.231747Epoch 456 		 Validation Loss: 1.263153, 		 Training Loss: 1.240121Epoch 457 		 Validation Loss: 1.263127, 		 Training Loss: 1.243573Epoch 458 		 Validation Loss: 1.263161, 		 Training Loss: 1.236606Validation Loss Decreased(70.734936--->70.733684) Saving model ...
Epoch 459 		 Validation Loss: 1.263101, 		 Training Loss: 1.237220Validation Loss Decreased(70.733684--->70.732720) Saving model ...
Epoch 460 		 Validation Loss: 1.263084, 		 Training Loss: 1.244763Validation Loss Decreased(70.732720--->70.732060) Saving model ...
Epoch 461 		 Validation Loss: 1.263072, 		 Training Loss: 1.241260Validation Loss Decreased(70.732060--->70.731833) Saving model ...
Epoch 462 		 Validation Loss: 1.263068, 		 Training Loss: 1.243788Validation Loss Decreased(70.731833--->70.731326) Saving model ...
Epoch 463 		 Validation Loss: 1.263059, 		 Training Loss: 1.243372Validation Loss Decreased(70.731326--->70.730666) Saving model ...
Epoch 464 		 Validation Loss: 1.263048, 		 Training Loss: 1.247949Epoch 465 		 Validation Loss: 1.263083, 		 Training Loss: 1.236100Validation Loss Decreased(70.730666--->70.729301) Saving model ...
Epoch 466 		 Validation Loss: 1.263023, 		 Training Loss: 1.250439Validation Loss Decreased(70.729301--->70.728418) Saving model ...
Epoch 467 		 Validation Loss: 1.263007, 		 Training Loss: 1.242328Validation Loss Decreased(70.728418--->70.728384) Saving model ...
Epoch 468 		 Validation Loss: 1.263007, 		 Training Loss: 1.239491Validation Loss Decreased(70.728384--->70.727513) Saving model ...
Epoch 469 		 Validation Loss: 1.262991, 		 Training Loss: 1.250959Validation Loss Decreased(70.727513--->70.726284) Saving model ...
Epoch 470 		 Validation Loss: 1.262969, 		 Training Loss: 1.247741Epoch 471 		 Validation Loss: 1.262974, 		 Training Loss: 1.238830Epoch 472 		 Validation Loss: 1.263031, 		 Training Loss: 1.235748Validation Loss Decreased(70.726284--->70.725917) Saving model ...
Epoch 473 		 Validation Loss: 1.262963, 		 Training Loss: 1.242237Validation Loss Decreased(70.725917--->70.725341) Saving model ...
Epoch 474 		 Validation Loss: 1.262953, 		 Training Loss: 1.239172Validation Loss Decreased(70.725341--->70.724753) Saving model ...
Epoch 475 		 Validation Loss: 1.262942, 		 Training Loss: 1.243168Validation Loss Decreased(70.724753--->70.724510) Saving model ...
Epoch 476 		 Validation Loss: 1.262938, 		 Training Loss: 1.241161Validation Loss Decreased(70.724510--->70.724298) Saving model ...
Epoch 477 		 Validation Loss: 1.262934, 		 Training Loss: 1.252632Validation Loss Decreased(70.724298--->70.723630) Saving model ...
Epoch 478 		 Validation Loss: 1.262922, 		 Training Loss: 1.244178Epoch 479 		 Validation Loss: 1.262924, 		 Training Loss: 1.235107Validation Loss Decreased(70.723630--->70.720721) Saving model ...
Epoch 480 		 Validation Loss: 1.262870, 		 Training Loss: 1.271063Epoch 481 		 Validation Loss: 1.262872, 		 Training Loss: 1.248925Validation Loss Decreased(70.720721--->70.718899) Saving model ...
Epoch 482 		 Validation Loss: 1.262837, 		 Training Loss: 1.249372Epoch 483 		 Validation Loss: 1.262839, 		 Training Loss: 1.236868Epoch 484 		 Validation Loss: 1.262851, 		 Training Loss: 1.228458Epoch 485 		 Validation Loss: 1.262881, 		 Training Loss: 1.240929Epoch 486 		 Validation Loss: 1.262843, 		 Training Loss: 1.247770Validation Loss Decreased(70.718899--->70.718208) Saving model ...
Epoch 487 		 Validation Loss: 1.262825, 		 Training Loss: 1.248481Epoch 488 		 Validation Loss: 1.262852, 		 Training Loss: 1.246425Epoch 489 		 Validation Loss: 1.262832, 		 Training Loss: 1.231209Validation Loss Decreased(70.718208--->70.716762) Saving model ...
Epoch 490 		 Validation Loss: 1.262799, 		 Training Loss: 1.253936Validation Loss Decreased(70.716762--->70.715564) Saving model ...
Epoch 491 		 Validation Loss: 1.262778, 		 Training Loss: 1.249680Epoch 492 		 Validation Loss: 1.262811, 		 Training Loss: 1.249174Validation Loss Decreased(70.715564--->70.714197) Saving model ...
Epoch 493 		 Validation Loss: 1.262754, 		 Training Loss: 1.242888Epoch 494 		 Validation Loss: 1.262757, 		 Training Loss: 1.232544Epoch 495 		 Validation Loss: 1.262754, 		 Training Loss: 1.243169Epoch 496 		 Validation Loss: 1.262791, 		 Training Loss: 1.232255Validation Loss Decreased(70.714197--->70.714010) Saving model ...
Epoch 497 		 Validation Loss: 1.262750, 		 Training Loss: 1.248888Epoch 498 		 Validation Loss: 1.262765, 		 Training Loss: 1.234618Validation Loss Decreased(70.714010--->70.713933) Saving model ...
Epoch 499 		 Validation Loss: 1.262749, 		 Training Loss: 1.245722Validation Loss Decreased(70.713933--->70.713293) Saving model ...
Epoch 500 		 Validation Loss: 1.262737, 		 Training Loss: 1.239782                         Training Completed!
#-----------------------------------------------------------------------#
